---
title: "Exploring a simple linkage example"
format: html
---

The Python package implements the Bloom filter linkage method ([Schnell et al., 2009](https://bmcmedinformdecismak.biomedcentral.com/articles/10.1186/1472-6947-9-41)), and can also implement pretrained Hash embeddings ([Miranda et al., 2022](https://arxiv.org/abs/2212.09255)), if a suitable large, pre-matched corpus of data is available.

Let us consider a small example where we want to link two excerpts of data on
bands. In this scenario, we are looking at some toy data on the members of a
fictional, German rock trio called "VerknÃ¼pfung". In this example we will see how to use untrained Bloom filters to match data.

### Loading the data

First, we load our data into `pandas.DataFrame` objects. Here, the first
records align, but the other two records should be swapped to have an aligned
matching. We will use the toolkit to identify these matches.

```{python}
import pandas as pd

df1 = pd.DataFrame(
    {
        "first_name": ["Laura", "Kaspar", "Grete"],
        "last_name": ["Daten", "Gorman", "Knopf"],
        "gender": ["f", "m", "f"],
        "date_of_birth": ["01/03/1977", "31/12/1975", "12/7/1981"],
        "instrument": ["bass", "guitar", "drums"],
    }
)
df2 = pd.DataFrame(
    {
        "name": ["Laura Datten", "Greta Knopf", "Casper Goreman"],
        "sex": ["female", "female", "male"],
        "main_instrument": ["bass guitar", "percussion", "electric guitar"],
        "birth_date": ["1977-03-23", "1981-07-12", "1975-12-31"],
    }
)
```

> [!NOTE]
> These datasets don't have the same column names or follow the same encodings,
> and there are several spelling mistakes in the names of the band members, as well as a typo in the dates.
>
> Thankfully, the `pprl_toolkit` is flexible enough to handle this!

### Creating and assigning a feature factory

The next step is to decide how to process each of the columns in our datasets.

To do this, we define a feature factory that maps column types to feature
generation functions, and a column specification for each dataset mapping our
columns to column types in the factory.

```{python}
from pprl.embedder import features
from functools import partial

factory = dict(
    name=features.gen_name_features,
    sex=features.gen_sex_features,
    misc=features.gen_misc_features,
    dob=features.gen_dateofbirth_features,
    instrument=partial(features.gen_misc_shingled_features, label="instrument")
)
spec1 = dict(
    first_name="name",
    last_name="name",
    gender="sex",
    instrument="instrument",
    date_of_birth="dob",
)
spec2 = dict(name="name", sex="sex", main_instrument="instrument", birth_date="dob")
```

> [!TIP]
> The feature generation functions, `features.gen_XXX_features` have sensible default parameters, but sometimes have to be passed in to the feature factory with different parameters, such as to set a feature label in the example above.
> There are two ways to achieve this. Either use `functools.partial` to set parameters (as above), or pass keyword arguments as a dictionary of dictionaries to the `Embedder` as `ff_args`.

### Embedding the data

With our specifications sorted out, we can get to creating our Bloom filter
embedding. Before doing so, we need to decide on two parameters: the size of
the filter and the number of hashes. By default, these are `1024` and `2`,
respectively.

Once we've decided, we can create our `Embedder` instance and use it to embed
our data with their column specifications.

```{python}
from pprl.embedder.embedder import Embedder

embedder = Embedder(factory, bf_size=1024, num_hashes=2)
edf1 = embedder.embed(df1, colspec=spec1, update_thresholds=True)
edf2 = embedder.embed(df2, colspec=spec2, update_thresholds=True)
```

If we take a look at one of these embedded datasets, we can see that it has a
whole bunch of new columns. There is a `_features` column for each of the
original columns containing their pre-embedding string features, and there's an `all_features` column that combines the features. Then there are
three additional columns: `bf_indices`, `bf_norms` and `thresholds`.

```{python}
edf1.columns
```

The `bf_indices` column contains the Bloom filters, represented compactly as a list of non-zero indices for each record. The `bf_norms` column contains the norm of each Bloom filter with respect to the Soft Cosine Measure (SCM) matrix. In this case since we are using an untrained model, the SCM matrix is an identity matrix, and the norm is just the Euclidean norm of the Bloom filter represented as a binary vector, which is equal to `np.sqrt(len(bf_indices[i]))` for record `i`. The norm is used to scale the similarity measures so that they take values between -1 and 1.

The `thresholds` column is calculated to provide, for each record, a threshold similarity score below which it will not be matched. It's like a reserve price in an auction -- it stops a record being matched to another record when the similarity isn't high enough. In this feature, the method implemented here differs from other linkage methods, which typically only have one global threshold score for the entire dataset.

<!-- ToDO: Write an explainer on the threshold method, and link it here -->

### The processed features

Let's take a look at how the features are processed into small text strings (shingles) before being hashed into the Bloom filter. The first record in the first dataset is the same person as the first record in the second dataset, although the data is not identical, so we can compare the processed features for these records to see how `pprl_toolkit` puts them into a format where they can be compared.

First, we'll look at date of birth:

```{python}
print(edf1.date_of_birth_features[0])
print(edf2.birth_date_features[0])
```

Python can parse the different formats easily. Although the dates are slightly different in the dataset, the year and month will still match, even though the day will not.

Then we'll look at name:

```{python}
print(edf1.first_name_features[0] + edf1.last_name_features[0])
print(edf2.name_features[0])
```

The two datasets store the names differently, but this doesn't matter for the Bloom filter method because it treats each record like a bag of features. By default, the name processor produces 2-grams, 3-grams and 4-grams.

The sex processing function just converts different formats to lowercase and takes the first letter. This will often be enough:

```{python}
print(edf1.gender_features[0])
print(edf2.sex_features[0])
```


Finally, we'll see how our instrument feature function (`partial(features.gen_misc_shingled_features, label="instrument")`) processed the data:

```{python}
print(edf1.instrument_features[0])
print(edf2.main_instrument_features[0])
```

Setting the `label` argument was important to ensure that the shingles match (and are hashed to the same slots) because the default behaviour of the function is to use the column name as a label: since the two columns have different names, the default wouldn't have allowed the features to match to each other.

### Performing the linkage

We can now perform the linkage by comparing these Bloom filter embeddings. We
use the Soft Cosine Measure (which in this untrained model, is equivalent to a normal cosine similarity metric) to calculate record-wise similarity and an adapted
Hungarian algorithm to match the records based on those similarities.

```{python}
similarities = embedder.compare(edf1, edf2)
similarities
```

This `SimilarityArray` object is an augmented `numpy.ndarray` that can perform
our matching. The matching itself can optionally be called with an absolute threshold score, but it doesn't need one.

```{python}
matching = similarities.match()
matching
```

So, all three of the records in each dataset were matched correctly. Excellent!
