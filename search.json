[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to the pprl documentation!",
    "section": "",
    "text": "This package, pprl, implements a method for performing Privacy Preserving Record Linkage. This linkage can be done locally or through Google Cloud Platform.",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "index.html#what-is-this-and-why-does-it-exist",
    "href": "index.html#what-is-this-and-why-does-it-exist",
    "title": "Welcome to the pprl documentation!",
    "section": "",
    "text": "This package, pprl, implements a method for performing Privacy Preserving Record Linkage. This linkage can be done locally or through Google Cloud Platform.",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "index.html#where-do-i-go-now",
    "href": "index.html#where-do-i-go-now",
    "title": "Welcome to the pprl documentation!",
    "section": "Where do I go now?",
    "text": "Where do I go now?\nIf you’re looking to get stuck in with pprl, head over to our tutorials.\nFor more focused, technical details of how this all works, see our API reference.",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "docs/reference/index.html",
    "href": "docs/reference/index.html",
    "title": "API reference",
    "section": "",
    "text": "Tools for generating a Bloom filter embedding and its underlying features.\n\n\n\nbloom_filters\nModule for the Bloom filter encoder.\n\n\nembedder\nClasses and functions for handling embedding objects.\n\n\nfeatures\nFeature generation functions for various column types.\n\n\n\n\n\n\nFunctions for handling the data and key encryption processes.\n\n\n\nencryption\nTools for performing envelope encryption on GCP.\n\n\n\n\n\n\nFunctions for working out and handling linkage configuration.\n\n\n\nconfig\nFunctions for handling PPRL configuration.\n\n\n\n\n\n\nFunctions for the Flask application where users upload, process, and download their data.\n\n\n\nutils\nUtility functions for the party-side app.\n\n\n\n\n\n\nFunctions for the matching workload server. Used in scripts/server.py\n\n\n\ncloud\nFunctions for performing matching in the cloud.\n\n\nlocal\nFunctions for performing matching locally.\n\n\nperform\nFunctions for performing the matching itself.",
    "crumbs": [
      "About",
      "Docs",
      "API reference"
    ]
  },
  {
    "objectID": "docs/reference/index.html#embeddings",
    "href": "docs/reference/index.html#embeddings",
    "title": "API reference",
    "section": "",
    "text": "Tools for generating a Bloom filter embedding and its underlying features.\n\n\n\nbloom_filters\nModule for the Bloom filter encoder.\n\n\nembedder\nClasses and functions for handling embedding objects.\n\n\nfeatures\nFeature generation functions for various column types.",
    "crumbs": [
      "About",
      "Docs",
      "API reference"
    ]
  },
  {
    "objectID": "docs/reference/index.html#encryption",
    "href": "docs/reference/index.html#encryption",
    "title": "API reference",
    "section": "",
    "text": "Functions for handling the data and key encryption processes.\n\n\n\nencryption\nTools for performing envelope encryption on GCP.",
    "crumbs": [
      "About",
      "Docs",
      "API reference"
    ]
  },
  {
    "objectID": "docs/reference/index.html#configuration",
    "href": "docs/reference/index.html#configuration",
    "title": "API reference",
    "section": "",
    "text": "Functions for working out and handling linkage configuration.\n\n\n\nconfig\nFunctions for handling PPRL configuration.",
    "crumbs": [
      "About",
      "Docs",
      "API reference"
    ]
  },
  {
    "objectID": "docs/reference/index.html#client-side-app",
    "href": "docs/reference/index.html#client-side-app",
    "title": "API reference",
    "section": "",
    "text": "Functions for the Flask application where users upload, process, and download their data.\n\n\n\nutils\nUtility functions for the party-side app.",
    "crumbs": [
      "About",
      "Docs",
      "API reference"
    ]
  },
  {
    "objectID": "docs/reference/index.html#server-functions",
    "href": "docs/reference/index.html#server-functions",
    "title": "API reference",
    "section": "",
    "text": "Functions for the matching workload server. Used in scripts/server.py\n\n\n\ncloud\nFunctions for performing matching in the cloud.\n\n\nlocal\nFunctions for performing matching locally.\n\n\nperform\nFunctions for performing the matching itself.",
    "crumbs": [
      "About",
      "Docs",
      "API reference"
    ]
  },
  {
    "objectID": "docs/reference/config.html",
    "href": "docs/reference/config.html",
    "title": "config",
    "section": "",
    "text": "config\nFunctions for handling PPRL configuration.\n\n\n\n\n\nName\nDescription\n\n\n\n\nload_environment\nLoad the configuration file as a dictionary.\n\n\n\n\n\nconfig.load_environment(path=None)\nLoad the configuration file as a dictionary.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npath\nstr\nLocation of the configuration file to load. If not specified, try to load the configuration file from the root of the pprl installation called .env.\nNone\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\ncollections.collections.OrderedDict\nMapping of the key-value pairs in the configuration file.",
    "crumbs": [
      "About",
      "Docs",
      "API reference",
      "config"
    ]
  },
  {
    "objectID": "docs/reference/config.html#functions",
    "href": "docs/reference/config.html#functions",
    "title": "config",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nload_environment\nLoad the configuration file as a dictionary.\n\n\n\n\n\nconfig.load_environment(path=None)\nLoad the configuration file as a dictionary.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npath\nstr\nLocation of the configuration file to load. If not specified, try to load the configuration file from the root of the pprl installation called .env.\nNone\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\ncollections.collections.OrderedDict\nMapping of the key-value pairs in the configuration file.",
    "crumbs": [
      "About",
      "Docs",
      "API reference",
      "config"
    ]
  },
  {
    "objectID": "docs/reference/cloud.html",
    "href": "docs/reference/cloud.html",
    "title": "cloud",
    "section": "",
    "text": "matching.cloud\nFunctions for performing matching in the cloud.\n\n\n\n\n\nName\nDescription\n\n\n\n\ncreate_impersonation_credentials\nCreate credentials from an identity pool for impersonating a party.\n\n\ndownload_embedder\nDownload and initiate the embedder from those on GCP.\n\n\ndownload_party_assets\nDownload the encrypted data and DEK for a party from GCP.\n\n\nprepare_party_assets\nDownload and decrypt the assets for a party from GCP.\n\n\nupload_party_results\nEncrypt and upload a party’s results to GCP.\n\n\n\n\n\nmatching.cloud.create_impersonation_credentials(party, operator)\nCreate credentials from an identity pool for impersonating a party.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nparty\nstr\nName of the party to impersonate.\nrequired\n\n\noperator\nstr\nName of the workload operator.\nrequired\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\ngoogle.google.auth.google.auth.identity_pool.google.auth.identity_pool.Credentials\nCredentials created using the party attestation verifier.\n\n\n\n\n\n\n\nmatching.cloud.download_embedder(parties, operator)\nDownload and initiate the embedder from those on GCP.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nparties\nlist[str]\nList of data-owning party names.\nrequired\n\n\noperator\nstr\nName of the workload operator.\nrequired\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\npprl.embedder.embedder.Embedder\nReformed embedder instance.\n\n\n\n\n\n\n\nmatching.cloud.download_party_assets(store, party)\nDownload the encrypted data and DEK for a party from GCP.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nstore\ngoogle.google.cloud.google.cloud.storage.google.cloud.storage.Client\nGCP storage client using identity pool credentials.\nrequired\n\n\nparty\nstr\nName of the party.\nrequired\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nbytes\nEncrypted data frame for linkage.\n\n\nbytes\nEncrypted data encryption key (used to encrypt the data).\n\n\n\n\n\n\n\nmatching.cloud.prepare_party_assets(party, operator, location, version)\nDownload and decrypt the assets for a party from GCP.\nTo enable these steps, we must first impersonate the party service account via the workload identity pool we created during project set-up.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nparty\nstr\nName of the party.\nrequired\n\n\noperator\nstr\nName of the workload operator.\nrequired\n\n\nlocation\nstr\nLocation of the party’s workload identity pool and keyring on GCP.\nrequired\n\n\nversion\nint | str\nKey version to retrieve for party asymmetric key encryption key.\nrequired\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\npandas.pandas.DataFrame\nDecrypted data frame for linkage.\n\n\nbytes\nDecrypted data encryption key.\n\n\n\n\n\n\n\nmatching.cloud.upload_party_results(output, dek, party, operator)\nEncrypt and upload a party’s results to GCP.\nLike prepare_party_assets, we must first impersonate the party service account to access their storage bucket.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\noutput\npandas.pandas.DataFrame\nParty’s output from the matching.\nrequired\n\n\ndek\nbytes\nData encryption key.\nrequired\n\n\nparty\nstr\nName of the party whose results are being processed.\nrequired\n\n\noperator\nstr\nName of the workload operator.\nrequired",
    "crumbs": [
      "About",
      "Docs",
      "API reference",
      "cloud"
    ]
  },
  {
    "objectID": "docs/reference/cloud.html#functions",
    "href": "docs/reference/cloud.html#functions",
    "title": "cloud",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\ncreate_impersonation_credentials\nCreate credentials from an identity pool for impersonating a party.\n\n\ndownload_embedder\nDownload and initiate the embedder from those on GCP.\n\n\ndownload_party_assets\nDownload the encrypted data and DEK for a party from GCP.\n\n\nprepare_party_assets\nDownload and decrypt the assets for a party from GCP.\n\n\nupload_party_results\nEncrypt and upload a party’s results to GCP.\n\n\n\n\n\nmatching.cloud.create_impersonation_credentials(party, operator)\nCreate credentials from an identity pool for impersonating a party.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nparty\nstr\nName of the party to impersonate.\nrequired\n\n\noperator\nstr\nName of the workload operator.\nrequired\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\ngoogle.google.auth.google.auth.identity_pool.google.auth.identity_pool.Credentials\nCredentials created using the party attestation verifier.\n\n\n\n\n\n\n\nmatching.cloud.download_embedder(parties, operator)\nDownload and initiate the embedder from those on GCP.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nparties\nlist[str]\nList of data-owning party names.\nrequired\n\n\noperator\nstr\nName of the workload operator.\nrequired\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\npprl.embedder.embedder.Embedder\nReformed embedder instance.\n\n\n\n\n\n\n\nmatching.cloud.download_party_assets(store, party)\nDownload the encrypted data and DEK for a party from GCP.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nstore\ngoogle.google.cloud.google.cloud.storage.google.cloud.storage.Client\nGCP storage client using identity pool credentials.\nrequired\n\n\nparty\nstr\nName of the party.\nrequired\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nbytes\nEncrypted data frame for linkage.\n\n\nbytes\nEncrypted data encryption key (used to encrypt the data).\n\n\n\n\n\n\n\nmatching.cloud.prepare_party_assets(party, operator, location, version)\nDownload and decrypt the assets for a party from GCP.\nTo enable these steps, we must first impersonate the party service account via the workload identity pool we created during project set-up.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nparty\nstr\nName of the party.\nrequired\n\n\noperator\nstr\nName of the workload operator.\nrequired\n\n\nlocation\nstr\nLocation of the party’s workload identity pool and keyring on GCP.\nrequired\n\n\nversion\nint | str\nKey version to retrieve for party asymmetric key encryption key.\nrequired\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\npandas.pandas.DataFrame\nDecrypted data frame for linkage.\n\n\nbytes\nDecrypted data encryption key.\n\n\n\n\n\n\n\nmatching.cloud.upload_party_results(output, dek, party, operator)\nEncrypt and upload a party’s results to GCP.\nLike prepare_party_assets, we must first impersonate the party service account to access their storage bucket.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\noutput\npandas.pandas.DataFrame\nParty’s output from the matching.\nrequired\n\n\ndek\nbytes\nData encryption key.\nrequired\n\n\nparty\nstr\nName of the party whose results are being processed.\nrequired\n\n\noperator\nstr\nName of the workload operator.\nrequired",
    "crumbs": [
      "About",
      "Docs",
      "API reference",
      "cloud"
    ]
  },
  {
    "objectID": "docs/reference/embedder.html",
    "href": "docs/reference/embedder.html",
    "title": "embedder",
    "section": "",
    "text": "embedder.embedder\nClasses and functions for handling embedding objects.\n\n\n\n\n\nName\nDescription\n\n\n\n\nEmbeddedDataFrame\nA data frame with a reference to an Embedder object.\n\n\nEmbedder\nClass for embedding a dataset.\n\n\nSimilarityArray\nAugmented NumPy array of similarity scores with extra attributes.\n\n\n\n\n\nembedder.embedder.EmbeddedDataFrame(self, data, embedder, update_norms=True, update_thresholds=False, *args, **kwargs)\nA data frame with a reference to an Embedder object.\nAn EmbeddedDataFrame (EDF) instance wraps together a pandas.DataFrame with a reference to a pprl.embedder.Embedder object. An EDF also has a mandatory bf_indices column, describing the Bloom filter indices used for linkage.\nThe EDF instance can also calculate bf_norms and thresholds columns which are used in the Embedder.compare() method to compute pprl.embedder.SimilarityArray instances.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndata\nnumpy.numpy.ndarray | typing.Iterable | dict | pandas.pandas.DataFrame\nData to which to attach the embedder. Must include a bf_indices column with list data type.\nrequired\n\n\nembedder\npprl.embedder.embedder.Embedder\nA compatible embedder object for the Bloom filter columns in data.\nrequired\n\n\nupdate_norms\nbool\nWhether to update the Bloom filter norms on creation. Defaults to False.\nTrue\n\n\nupdate_thresholds\nbool\nWhether to update the similarity thresholds on creation. Defaults to True.\nFalse\n\n\n*args\n\nAdditional positional arguments to pass to pandas.DataFrame along with data.\n()\n\n\n**kwargs\n\nAdditional keyword arguments to pass to pandas.DataFrame along with data.\n{}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nembedder_checksum\nstr\nHexadecimal string digest from self.embedder.\n\n\n\n\n\n\nAn EDF instance is usually created from an existing Embedder object by calling the embedder.embed() method. It can also be initialised using an embedder and a pandas.DataFrame that already has a bf_indices column via EmbeddedDataFrame(df, embedder).\nIf using the second method it is up to the user to ensure that the Embedder instance is compatible with the bf_indices column (as well as bf_norms and thresholds, if present) in the data frame. If in doubt, call edf.update_norms() and edf.update_thresholds() to refresh them.\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nanonymise\nRemove raw data from embedded dataframe.\n\n\nto_bloom_matrix\nConvert Bloom filter indices into a binary matrix.\n\n\nupdate_norms\nGenerate vector norms for each row.\n\n\nupdate_thresholds\nGenerate matching thresholds for each row of the data.\n\n\n\n\n\nembedder.embedder.EmbeddedDataFrame.anonymise(keep=None)\nRemove raw data from embedded dataframe.\nRemove all columns from the embedded dataframe expect columns listed in keep and bf_indices, bf_norms and thresholds.\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nlist[str]\nColumns to be returned as they appear in the data in addition to bf_indices, bf_norms and thresholds if they are present in the data.\n\n\n\n\n\n\n\nembedder.embedder.EmbeddedDataFrame.to_bloom_matrix()\nConvert Bloom filter indices into a binary matrix.\nThe matrix has a row for each row in the EDF. The number of columns is equal to self.embedder.bf_size + self.embedder.offset. Each row in the matrix is a Bloom filter expressed as a binary vector, with the ones corresponding to hashed features. This representation is used in the Embedder.compare() method.\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nnumpy.numpy.ndarray\nBinary array of size (len(self), self.embedder.bf_size + self.embedder.offset).\n\n\n\n\n\n\n\nembedder.embedder.EmbeddedDataFrame.update_norms()\nGenerate vector norms for each row.\nCreate or update the bf_norms column in the EDF. This method calculates, for each Bloom filter, its Euclidean norm when the filter is expressed as a binary vector, and saves it to the EDF. The norm is used to scale the (Soft) Cosine similarity scores.\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\ndata.bf_norms\nlist\nColumn of vector norms for each row in the EDF.\n\n\n\n\n\n\n\nembedder.embedder.EmbeddedDataFrame.update_thresholds()\nGenerate matching thresholds for each row of the data.\nThe threshold is the minimum similarity score that will be matched. It is found by getting the pairwise similarities between each row and the other rows in the same EDF, and taking the maximum of these.\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\ndata.thresholds\nnumpy.numpy.ndarray\nColumn for maximum similarity of each row within the EDF.\n\n\n\n\n\n\n\n\n\nembedder.embedder.Embedder(self, feature_factory, ff_args=None, bf_size=1024, num_hashes=2, offset=0, salt=None)\nClass for embedding a dataset.\nEach instance of the Embedder class represents an embedding space on personal data features. An Embedder instance is defined by three things:\n\nA set of Bloom filter parameters\nA set of feature factory functions\nAn embedding matrix that corresponds to the above\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfeature_factory\ndict\nMapping from dataset columns to feature generation functions.\nrequired\n\n\nff_args\ndict[str, dict] | None\nMapping from dataset columns to keyword arguments for their respective feature generation functions.\nNone\n\n\nbf_size\nint\nSize of the Bloom filter. Default is 1024.\n1024\n\n\nnum_hashes\nint\nNumber of hashes to perform. Default is two.\n2\n\n\noffset\nint\nOffset for Bloom filter to enable masking. Default is zero.\n0\n\n\nsalt\nstr | None\nCryptographic salt added to tokens from the data before hashing.\nNone\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nscm_matrix\nnumpy.numpy.ndarray\nSoft Cosine Measure matrix. Initialised as an identity matrix of size bf_size + offset.\n\n\nfreq_matr_matched\nnumpy.numpy.ndarray\nMatched frequency matrix for computing scm_matrix. Initialised as an identity matrix of size bf_size + offset.\n\n\nfreq_matr_unmatched\nnumpy.numpy.ndarray\nUnmatched frequency matrix for computing scm_matrix. Initialised as an identity matrix of size bf_size + offset.\n\n\nchecksum\nstr\nHexadecimal string digest of the feature factory, SCM matrix, and other embedding parameters. Used to check an embedder is compatible with an EmbeddedDataFrame.\n\n\n\n\n\n\nWhen an instance is initialised in code, the embedding matrix is initialised as an identity matrix; the matrix can then be trained using a pair of datasets with known match status and the trained Embedder instance pickled to file. The pre-trained Embedder instance can then be reinitialised from the pickle file.\nBoth the untrained and trained instances provide embed() and compare() methods. Comparing datasets using an untrained Embedder instance is equivalent to calculating Cosine similarities on ordinary Bloom filters. Comparing datasets using a pre-trained Embedder calculates the Soft Cosine Measure between Bloom filters. The Soft Cosine Measure embedding matrix is trained using an experimental method.\n\n\n\n\n\n\nName\nDescription\n\n\n\n\ncompare\nCalculate a SimilarityArray on two EDFs.\n\n\nembed\nEncode data columns into features from Bloom embedding.\n\n\nfrom_pickle\nInitialise Embedder instance from pickle file.\n\n\nto_pickle\nSave Embedder instance to pickle file.\n\n\ntrain\nFit Soft Cosine Measure matrix to two matched datasets.\n\n\n\n\n\nembedder.embedder.Embedder.compare(edf1, edf2, require_thresholds=True)\nCalculate a SimilarityArray on two EDFs.\nGiven two EDFs, calculate all pairwise Soft Cosine Similarities between rows.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nedf1\npprl.embedder.embedder.EmbeddedDataFrame\nAn EDF instance with N rows. Must have thresholds column unless require_thresholds=False.\nrequired\n\n\nedf2\npprl.embedder.embedder.EmbeddedDataFrame\nAn EDF instance with M rows. Must have thresholds column unless require_thresholds=False.\nrequired\n\n\nrequire_thresholds\nbool\nIf True (default), the comparison will fail if thresholds are not present. Must be explicitly set to False to allow comparison without thresholds.\nTrue\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\npprl.embedder.embedder.SimilarityArray\nAn N by M array containing the similarity matrix of pairwise Soft Cosine similarities between rows of edf1 and edf2.\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nValueError\nIf require_thresholds is True and both EDFs don’t have a thresholds column.\n\n\n\n\n\n\n\nembedder.embedder.Embedder.embed(df, colspec, update_norms=True, update_thresholds=False)\nEncode data columns into features from Bloom embedding.\nGiven a pandas DataFrame and a column specification, convert columns into string features, and then embed the features into Bloom filters. The method returns an instance of EmbeddedDataFrame, which is an augmented pandas DataFrame.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndf\npandas.pandas.DataFrame\nData frame to be embedded.\nrequired\n\n\ncolspec\ndict\nDictionary mapping columns in df to feature factory functions.\nrequired\n\n\nupdate_norms\nbool\nWhether to calculate vector norms for SCM and add to EDF. False by default.\nTrue\n\n\nupdate_thresholds\nbool\nWhether to calculate similarity thresholds and add to EDF. Used as an outside option in matching. False by default.\nFalse\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\npprl.embedder.embedder.EmbeddedDataFrame\nAn embedded data frame with its embedder.\n\n\n\n\n\n\n\nembedder.embedder.Embedder.from_pickle(path=None, pickled=None)\nInitialise Embedder instance from pickle file.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npath\nstr\nFile path from which to load the pickled embedder.\nNone\n\n\npickled\nbytes\nByte-string containing the pickled embedder.\nNone\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nValueError\nIf not exactly one of path and pickled are specified.\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\npprl.embedder.embedder.Embedder\nThe reformed instance of the Embedder class.\n\n\n\n\n\n\n\nembedder.embedder.Embedder.to_pickle(path=None)\nSave Embedder instance to pickle file.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npath\nstr\nFile path at which to save the pickled embedder. If not specified, the pickled bytes string is returned.\nNone\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nbytes or None\nIf path is not specified, the pickled string comes back. Otherwise, nothing is returned.\n\n\n\n\n\n\n\nembedder.embedder.Embedder.train(edf1, edf2, update=True, learning_rate=1.0, eps=0.01, random_state=None)\nFit Soft Cosine Measure matrix to two matched datasets.\nThis function updates the scm_matrix attribute in-place along with its constituent matrices, freq_matr_matched and freq_matr_unmatched.\nProvide two datasets of pre-matched data, with matching records aligned. If update=True, the training is cumulative, so that train() can be called more than once, updating the same matrices each time by adding new frequency tables. Otherwise, all three matrices are reinitialised prior to training.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nedf1\npprl.embedder.embedder.EmbeddedDataFrame\nAn embedded dataset.\nrequired\n\n\nedf2\npprl.embedder.embedder.EmbeddedDataFrame\nAn Embedded dataset of known matches in the same order as edf1.\nrequired\n\n\nupdate\nbool\nWhether to update the existing SCM matrix, or overwrite it. Defaults to True.\nTrue\n\n\neps\nfloat\nSmall non-negative constant to avoid -Inf in log of frequencies. Default is one.\n0.01\n\n\nlearning_rate\nfloat\nScaling factor to dampen matrix updates. Must be in the interval (0, 1]. Default is 0.01.\n1.0\n\n\nrandom_state\nNone | numpy.numpy.random.numpy.random.RandomState\nRandom state to pass to dataset jumbler. Defaults to None.\nNone\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nscm_matrix\nnumpy.numpy.ndarray\nSoft Cosine Measure matrix that is fitted cumulatively or afresh.\n\n\n\n\n\n\n\n\n\nembedder.embedder.SimilarityArray()\nAugmented NumPy array of similarity scores with extra attributes.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ninput_array\n\nOriginal array of similarity score data.\nrequired\n\n\nthresholds\n\n2-tuple of similarity score thresholds for each axis. These thresholds are used when generating a matching.\nrequired\n\n\nembedder_checksum\n\nHexadecimal string digest of a pprl.embedder.Embedder object.\nrequired\n\n\n\n\n\n\nSimilarityArray objects are usually initialised from an instance of pprl.embedder.Embedder via the embedder.compare() method.\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nmatch\nCompute a matching.\n\n\n\n\n\nembedder.embedder.SimilarityArray.match(abs_cutoff=0, rel_cutoff=0, hungarian=True, require_thresholds=True)\nCompute a matching.\nGiven an array of similarity scores, compute a matching of its elements, using the Hungarian algorithm by default. If the SimilarityArray has thresholds, masking is used to ensure that prospective matches whose similarity score is below the thresholds are not returned. An abs_cutoff (global minimum similarity score) can also be supplied.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nabs_cutoff\nint or float\nA lower cutoff for the similarity score. No pairs with similarity below the absolute cutoff will be matched. By default, this is 0.\n0\n\n\nrel_cutoff\nint or float\nA margin above the row/column-specific threshold. Raises all thresholds by a constant. By default, this is 0.\n0\n\n\nhungarian\nbool\nWhether to compute the unique matching using the Hungarian algorithm, filtered using thresholds and abs_cutoff. Default is True. If False, just return all pairs above the threshold.\nTrue\n\n\nrequire_thresholds\nbool\nIf True (default), the matching will fail if thresholds is not present and valid. Must be explicitly set to False to allow matching without similarity thresholds.\nTrue\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\ntuple[list[int], list[int]]\n2-tuple of indexes containing row and column indices of matched pairs eg. ([0, 1, ...], [0, 1, ...]).\n\n\n\n\n\n\nIf hungarian=False, the matching returns all pairs with similarity score above the abs_cutoff, respecting thresholds if present. This method does not guarantee no duplicates.\n\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nnearest_pos_semi_definite\nCalculate nearest positive semi-definite version of a matrix.\n\n\n\n\n\nembedder.embedder.nearest_pos_semi_definite(X, eps=0.0)\nCalculate nearest positive semi-definite version of a matrix.\nThis function achieves this by setting all negative eigenvalues of the matrix to zero, or a small positive value to give a positive definite matrix.\nGraciously taken from this StackOverflow post\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nX\nnumpy.numpy.ndarray\nMatrix-like array.\nrequired\n\n\neps\nfloat\nUse a small positive constant to give a positive definite matrix. Default is 0 to give a positive semi-definite matrix.\n0.0\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nnumpy.numpy.ndarray\nA positive (semi-)definite matrix.",
    "crumbs": [
      "About",
      "Docs",
      "API reference",
      "embedder"
    ]
  },
  {
    "objectID": "docs/reference/embedder.html#classes",
    "href": "docs/reference/embedder.html#classes",
    "title": "embedder",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nEmbeddedDataFrame\nA data frame with a reference to an Embedder object.\n\n\nEmbedder\nClass for embedding a dataset.\n\n\nSimilarityArray\nAugmented NumPy array of similarity scores with extra attributes.\n\n\n\n\n\nembedder.embedder.EmbeddedDataFrame(self, data, embedder, update_norms=True, update_thresholds=False, *args, **kwargs)\nA data frame with a reference to an Embedder object.\nAn EmbeddedDataFrame (EDF) instance wraps together a pandas.DataFrame with a reference to a pprl.embedder.Embedder object. An EDF also has a mandatory bf_indices column, describing the Bloom filter indices used for linkage.\nThe EDF instance can also calculate bf_norms and thresholds columns which are used in the Embedder.compare() method to compute pprl.embedder.SimilarityArray instances.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndata\nnumpy.numpy.ndarray | typing.Iterable | dict | pandas.pandas.DataFrame\nData to which to attach the embedder. Must include a bf_indices column with list data type.\nrequired\n\n\nembedder\npprl.embedder.embedder.Embedder\nA compatible embedder object for the Bloom filter columns in data.\nrequired\n\n\nupdate_norms\nbool\nWhether to update the Bloom filter norms on creation. Defaults to False.\nTrue\n\n\nupdate_thresholds\nbool\nWhether to update the similarity thresholds on creation. Defaults to True.\nFalse\n\n\n*args\n\nAdditional positional arguments to pass to pandas.DataFrame along with data.\n()\n\n\n**kwargs\n\nAdditional keyword arguments to pass to pandas.DataFrame along with data.\n{}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nembedder_checksum\nstr\nHexadecimal string digest from self.embedder.\n\n\n\n\n\n\nAn EDF instance is usually created from an existing Embedder object by calling the embedder.embed() method. It can also be initialised using an embedder and a pandas.DataFrame that already has a bf_indices column via EmbeddedDataFrame(df, embedder).\nIf using the second method it is up to the user to ensure that the Embedder instance is compatible with the bf_indices column (as well as bf_norms and thresholds, if present) in the data frame. If in doubt, call edf.update_norms() and edf.update_thresholds() to refresh them.\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nanonymise\nRemove raw data from embedded dataframe.\n\n\nto_bloom_matrix\nConvert Bloom filter indices into a binary matrix.\n\n\nupdate_norms\nGenerate vector norms for each row.\n\n\nupdate_thresholds\nGenerate matching thresholds for each row of the data.\n\n\n\n\n\nembedder.embedder.EmbeddedDataFrame.anonymise(keep=None)\nRemove raw data from embedded dataframe.\nRemove all columns from the embedded dataframe expect columns listed in keep and bf_indices, bf_norms and thresholds.\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nlist[str]\nColumns to be returned as they appear in the data in addition to bf_indices, bf_norms and thresholds if they are present in the data.\n\n\n\n\n\n\n\nembedder.embedder.EmbeddedDataFrame.to_bloom_matrix()\nConvert Bloom filter indices into a binary matrix.\nThe matrix has a row for each row in the EDF. The number of columns is equal to self.embedder.bf_size + self.embedder.offset. Each row in the matrix is a Bloom filter expressed as a binary vector, with the ones corresponding to hashed features. This representation is used in the Embedder.compare() method.\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nnumpy.numpy.ndarray\nBinary array of size (len(self), self.embedder.bf_size + self.embedder.offset).\n\n\n\n\n\n\n\nembedder.embedder.EmbeddedDataFrame.update_norms()\nGenerate vector norms for each row.\nCreate or update the bf_norms column in the EDF. This method calculates, for each Bloom filter, its Euclidean norm when the filter is expressed as a binary vector, and saves it to the EDF. The norm is used to scale the (Soft) Cosine similarity scores.\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\ndata.bf_norms\nlist\nColumn of vector norms for each row in the EDF.\n\n\n\n\n\n\n\nembedder.embedder.EmbeddedDataFrame.update_thresholds()\nGenerate matching thresholds for each row of the data.\nThe threshold is the minimum similarity score that will be matched. It is found by getting the pairwise similarities between each row and the other rows in the same EDF, and taking the maximum of these.\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\ndata.thresholds\nnumpy.numpy.ndarray\nColumn for maximum similarity of each row within the EDF.\n\n\n\n\n\n\n\n\n\nembedder.embedder.Embedder(self, feature_factory, ff_args=None, bf_size=1024, num_hashes=2, offset=0, salt=None)\nClass for embedding a dataset.\nEach instance of the Embedder class represents an embedding space on personal data features. An Embedder instance is defined by three things:\n\nA set of Bloom filter parameters\nA set of feature factory functions\nAn embedding matrix that corresponds to the above\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfeature_factory\ndict\nMapping from dataset columns to feature generation functions.\nrequired\n\n\nff_args\ndict[str, dict] | None\nMapping from dataset columns to keyword arguments for their respective feature generation functions.\nNone\n\n\nbf_size\nint\nSize of the Bloom filter. Default is 1024.\n1024\n\n\nnum_hashes\nint\nNumber of hashes to perform. Default is two.\n2\n\n\noffset\nint\nOffset for Bloom filter to enable masking. Default is zero.\n0\n\n\nsalt\nstr | None\nCryptographic salt added to tokens from the data before hashing.\nNone\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nscm_matrix\nnumpy.numpy.ndarray\nSoft Cosine Measure matrix. Initialised as an identity matrix of size bf_size + offset.\n\n\nfreq_matr_matched\nnumpy.numpy.ndarray\nMatched frequency matrix for computing scm_matrix. Initialised as an identity matrix of size bf_size + offset.\n\n\nfreq_matr_unmatched\nnumpy.numpy.ndarray\nUnmatched frequency matrix for computing scm_matrix. Initialised as an identity matrix of size bf_size + offset.\n\n\nchecksum\nstr\nHexadecimal string digest of the feature factory, SCM matrix, and other embedding parameters. Used to check an embedder is compatible with an EmbeddedDataFrame.\n\n\n\n\n\n\nWhen an instance is initialised in code, the embedding matrix is initialised as an identity matrix; the matrix can then be trained using a pair of datasets with known match status and the trained Embedder instance pickled to file. The pre-trained Embedder instance can then be reinitialised from the pickle file.\nBoth the untrained and trained instances provide embed() and compare() methods. Comparing datasets using an untrained Embedder instance is equivalent to calculating Cosine similarities on ordinary Bloom filters. Comparing datasets using a pre-trained Embedder calculates the Soft Cosine Measure between Bloom filters. The Soft Cosine Measure embedding matrix is trained using an experimental method.\n\n\n\n\n\n\nName\nDescription\n\n\n\n\ncompare\nCalculate a SimilarityArray on two EDFs.\n\n\nembed\nEncode data columns into features from Bloom embedding.\n\n\nfrom_pickle\nInitialise Embedder instance from pickle file.\n\n\nto_pickle\nSave Embedder instance to pickle file.\n\n\ntrain\nFit Soft Cosine Measure matrix to two matched datasets.\n\n\n\n\n\nembedder.embedder.Embedder.compare(edf1, edf2, require_thresholds=True)\nCalculate a SimilarityArray on two EDFs.\nGiven two EDFs, calculate all pairwise Soft Cosine Similarities between rows.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nedf1\npprl.embedder.embedder.EmbeddedDataFrame\nAn EDF instance with N rows. Must have thresholds column unless require_thresholds=False.\nrequired\n\n\nedf2\npprl.embedder.embedder.EmbeddedDataFrame\nAn EDF instance with M rows. Must have thresholds column unless require_thresholds=False.\nrequired\n\n\nrequire_thresholds\nbool\nIf True (default), the comparison will fail if thresholds are not present. Must be explicitly set to False to allow comparison without thresholds.\nTrue\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\npprl.embedder.embedder.SimilarityArray\nAn N by M array containing the similarity matrix of pairwise Soft Cosine similarities between rows of edf1 and edf2.\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nValueError\nIf require_thresholds is True and both EDFs don’t have a thresholds column.\n\n\n\n\n\n\n\nembedder.embedder.Embedder.embed(df, colspec, update_norms=True, update_thresholds=False)\nEncode data columns into features from Bloom embedding.\nGiven a pandas DataFrame and a column specification, convert columns into string features, and then embed the features into Bloom filters. The method returns an instance of EmbeddedDataFrame, which is an augmented pandas DataFrame.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndf\npandas.pandas.DataFrame\nData frame to be embedded.\nrequired\n\n\ncolspec\ndict\nDictionary mapping columns in df to feature factory functions.\nrequired\n\n\nupdate_norms\nbool\nWhether to calculate vector norms for SCM and add to EDF. False by default.\nTrue\n\n\nupdate_thresholds\nbool\nWhether to calculate similarity thresholds and add to EDF. Used as an outside option in matching. False by default.\nFalse\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\npprl.embedder.embedder.EmbeddedDataFrame\nAn embedded data frame with its embedder.\n\n\n\n\n\n\n\nembedder.embedder.Embedder.from_pickle(path=None, pickled=None)\nInitialise Embedder instance from pickle file.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npath\nstr\nFile path from which to load the pickled embedder.\nNone\n\n\npickled\nbytes\nByte-string containing the pickled embedder.\nNone\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nValueError\nIf not exactly one of path and pickled are specified.\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\npprl.embedder.embedder.Embedder\nThe reformed instance of the Embedder class.\n\n\n\n\n\n\n\nembedder.embedder.Embedder.to_pickle(path=None)\nSave Embedder instance to pickle file.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npath\nstr\nFile path at which to save the pickled embedder. If not specified, the pickled bytes string is returned.\nNone\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nbytes or None\nIf path is not specified, the pickled string comes back. Otherwise, nothing is returned.\n\n\n\n\n\n\n\nembedder.embedder.Embedder.train(edf1, edf2, update=True, learning_rate=1.0, eps=0.01, random_state=None)\nFit Soft Cosine Measure matrix to two matched datasets.\nThis function updates the scm_matrix attribute in-place along with its constituent matrices, freq_matr_matched and freq_matr_unmatched.\nProvide two datasets of pre-matched data, with matching records aligned. If update=True, the training is cumulative, so that train() can be called more than once, updating the same matrices each time by adding new frequency tables. Otherwise, all three matrices are reinitialised prior to training.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nedf1\npprl.embedder.embedder.EmbeddedDataFrame\nAn embedded dataset.\nrequired\n\n\nedf2\npprl.embedder.embedder.EmbeddedDataFrame\nAn Embedded dataset of known matches in the same order as edf1.\nrequired\n\n\nupdate\nbool\nWhether to update the existing SCM matrix, or overwrite it. Defaults to True.\nTrue\n\n\neps\nfloat\nSmall non-negative constant to avoid -Inf in log of frequencies. Default is one.\n0.01\n\n\nlearning_rate\nfloat\nScaling factor to dampen matrix updates. Must be in the interval (0, 1]. Default is 0.01.\n1.0\n\n\nrandom_state\nNone | numpy.numpy.random.numpy.random.RandomState\nRandom state to pass to dataset jumbler. Defaults to None.\nNone\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nscm_matrix\nnumpy.numpy.ndarray\nSoft Cosine Measure matrix that is fitted cumulatively or afresh.\n\n\n\n\n\n\n\n\n\nembedder.embedder.SimilarityArray()\nAugmented NumPy array of similarity scores with extra attributes.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ninput_array\n\nOriginal array of similarity score data.\nrequired\n\n\nthresholds\n\n2-tuple of similarity score thresholds for each axis. These thresholds are used when generating a matching.\nrequired\n\n\nembedder_checksum\n\nHexadecimal string digest of a pprl.embedder.Embedder object.\nrequired\n\n\n\n\n\n\nSimilarityArray objects are usually initialised from an instance of pprl.embedder.Embedder via the embedder.compare() method.\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nmatch\nCompute a matching.\n\n\n\n\n\nembedder.embedder.SimilarityArray.match(abs_cutoff=0, rel_cutoff=0, hungarian=True, require_thresholds=True)\nCompute a matching.\nGiven an array of similarity scores, compute a matching of its elements, using the Hungarian algorithm by default. If the SimilarityArray has thresholds, masking is used to ensure that prospective matches whose similarity score is below the thresholds are not returned. An abs_cutoff (global minimum similarity score) can also be supplied.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nabs_cutoff\nint or float\nA lower cutoff for the similarity score. No pairs with similarity below the absolute cutoff will be matched. By default, this is 0.\n0\n\n\nrel_cutoff\nint or float\nA margin above the row/column-specific threshold. Raises all thresholds by a constant. By default, this is 0.\n0\n\n\nhungarian\nbool\nWhether to compute the unique matching using the Hungarian algorithm, filtered using thresholds and abs_cutoff. Default is True. If False, just return all pairs above the threshold.\nTrue\n\n\nrequire_thresholds\nbool\nIf True (default), the matching will fail if thresholds is not present and valid. Must be explicitly set to False to allow matching without similarity thresholds.\nTrue\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\ntuple[list[int], list[int]]\n2-tuple of indexes containing row and column indices of matched pairs eg. ([0, 1, ...], [0, 1, ...]).\n\n\n\n\n\n\nIf hungarian=False, the matching returns all pairs with similarity score above the abs_cutoff, respecting thresholds if present. This method does not guarantee no duplicates.",
    "crumbs": [
      "About",
      "Docs",
      "API reference",
      "embedder"
    ]
  },
  {
    "objectID": "docs/reference/embedder.html#functions",
    "href": "docs/reference/embedder.html#functions",
    "title": "embedder",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nnearest_pos_semi_definite\nCalculate nearest positive semi-definite version of a matrix.\n\n\n\n\n\nembedder.embedder.nearest_pos_semi_definite(X, eps=0.0)\nCalculate nearest positive semi-definite version of a matrix.\nThis function achieves this by setting all negative eigenvalues of the matrix to zero, or a small positive value to give a positive definite matrix.\nGraciously taken from this StackOverflow post\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nX\nnumpy.numpy.ndarray\nMatrix-like array.\nrequired\n\n\neps\nfloat\nUse a small positive constant to give a positive definite matrix. Default is 0 to give a positive semi-definite matrix.\n0.0\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nnumpy.numpy.ndarray\nA positive (semi-)definite matrix.",
    "crumbs": [
      "About",
      "Docs",
      "API reference",
      "embedder"
    ]
  },
  {
    "objectID": "docs/reference/encryption.html",
    "href": "docs/reference/encryption.html",
    "title": "encryption",
    "section": "",
    "text": "encryption\nTools for performing envelope encryption on GCP.\n\n\n\n\n\nName\nDescription\n\n\n\n\ndecrypt_data\nDecrypt a data frame with the provided key.\n\n\ndecrypt_dek\nDecrypt a data encryption key using an asymmetric key held on KMS.\n\n\nencrypt_data\nEncrypt a data frame.\n\n\nencrypt_dek\nEncrypt the data encryption key.\n\n\n\n\n\nencryption.decrypt_data(encrypted, key)\nDecrypt a data frame with the provided key.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nencrypted\nbytes\nData to be decrypted.\nrequired\n\n\nkey\nbytes\nKey used to encrypt the data.\nrequired\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\npandas.pandas.DataFrame\nDecrypted data frame.\n\n\n\n\n\n\n\nencryption.decrypt_dek(encrypted, party, location='global', version=1, **kwargs)\nDecrypt a data encryption key using an asymmetric key held on KMS.\nOwing to the nature of the encryption key set-up of pprl this function is only really to be used in the GCP Confidential Space set up by the linkage administrator.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nencrypted\nbytes\nKey to be decrypted.\nrequired\n\n\nparty\nstr\nName of the party whose key we are decrypting.\nrequired\n\n\nlocation\nstr\nLocation of the keyring on which the key lives.\n'global'\n\n\nversion\nint | str\nVersion of the asymmetric key to get from GCP. Default is 1.\n1\n\n\n**kwargs\ndict\nKeyword arguments to pass when creating an instance of google.cloud.kms.KeyManagementServiceClient.\n{}\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nbytes\nDecrypted data encryption key.\n\n\n\n\n\n\n\nencryption.encrypt_data(data, key=None)\nEncrypt a data frame.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndata\npandas.pandas.DataFrame\nDataframe to encrypt.\nrequired\n\n\nkey\nbytes\nFernet key to encrypt data frame. If not specified, create one.\nNone\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nbytes\nEncrypted data frame.\n\n\nbytes\nFernet key used to encrypt data frame.\n\n\n\n\n\n\n\nencryption.encrypt_dek(dek, party, location='global', version=1, **kwargs)\nEncrypt the data encryption key.\nWe encrypt the data encryption key using the public key portion of an asymmetric key retrieved from the GCP Key Management Service.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndek\nbytes\nData encryption key to be encrypted.\nrequired\n\n\nparty\nstr\nName of the party.\nrequired\n\n\nlocation\nstr\nLocation of the keyring on which the key lives.\n'global'\n\n\nversion\nint | str\nVersion of the asymmetric key to get from GCP. Default is 1.\n1\n\n\n**kwargs\ndict\nKeyword arguments to pass when creating an instance of google.cloud.kms.KeyManagementServiceClient.\n{}\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nbytes\nEncrypted data encryption key.",
    "crumbs": [
      "About",
      "Docs",
      "API reference",
      "encryption"
    ]
  },
  {
    "objectID": "docs/reference/encryption.html#functions",
    "href": "docs/reference/encryption.html#functions",
    "title": "encryption",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\ndecrypt_data\nDecrypt a data frame with the provided key.\n\n\ndecrypt_dek\nDecrypt a data encryption key using an asymmetric key held on KMS.\n\n\nencrypt_data\nEncrypt a data frame.\n\n\nencrypt_dek\nEncrypt the data encryption key.\n\n\n\n\n\nencryption.decrypt_data(encrypted, key)\nDecrypt a data frame with the provided key.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nencrypted\nbytes\nData to be decrypted.\nrequired\n\n\nkey\nbytes\nKey used to encrypt the data.\nrequired\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\npandas.pandas.DataFrame\nDecrypted data frame.\n\n\n\n\n\n\n\nencryption.decrypt_dek(encrypted, party, location='global', version=1, **kwargs)\nDecrypt a data encryption key using an asymmetric key held on KMS.\nOwing to the nature of the encryption key set-up of pprl this function is only really to be used in the GCP Confidential Space set up by the linkage administrator.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nencrypted\nbytes\nKey to be decrypted.\nrequired\n\n\nparty\nstr\nName of the party whose key we are decrypting.\nrequired\n\n\nlocation\nstr\nLocation of the keyring on which the key lives.\n'global'\n\n\nversion\nint | str\nVersion of the asymmetric key to get from GCP. Default is 1.\n1\n\n\n**kwargs\ndict\nKeyword arguments to pass when creating an instance of google.cloud.kms.KeyManagementServiceClient.\n{}\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nbytes\nDecrypted data encryption key.\n\n\n\n\n\n\n\nencryption.encrypt_data(data, key=None)\nEncrypt a data frame.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndata\npandas.pandas.DataFrame\nDataframe to encrypt.\nrequired\n\n\nkey\nbytes\nFernet key to encrypt data frame. If not specified, create one.\nNone\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nbytes\nEncrypted data frame.\n\n\nbytes\nFernet key used to encrypt data frame.\n\n\n\n\n\n\n\nencryption.encrypt_dek(dek, party, location='global', version=1, **kwargs)\nEncrypt the data encryption key.\nWe encrypt the data encryption key using the public key portion of an asymmetric key retrieved from the GCP Key Management Service.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndek\nbytes\nData encryption key to be encrypted.\nrequired\n\n\nparty\nstr\nName of the party.\nrequired\n\n\nlocation\nstr\nLocation of the keyring on which the key lives.\n'global'\n\n\nversion\nint | str\nVersion of the asymmetric key to get from GCP. Default is 1.\n1\n\n\n**kwargs\ndict\nKeyword arguments to pass when creating an instance of google.cloud.kms.KeyManagementServiceClient.\n{}\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nbytes\nEncrypted data encryption key.",
    "crumbs": [
      "About",
      "Docs",
      "API reference",
      "encryption"
    ]
  },
  {
    "objectID": "docs/tutorials/example-verknupfung.html",
    "href": "docs/tutorials/example-verknupfung.html",
    "title": "Exploring a simple linkage example",
    "section": "",
    "text": "The Python package implements the Bloom filter linkage method (Schnell et al., 2009), and can also implement pretrained Hash embeddings (Miranda et al., 2022), if a suitable large, pre-matched corpus of data is available.\nLet us consider a small example where we want to link two excerpts of data on bands. In this scenario, we are looking at some toy data on the members of a fictional, German rock trio called “Verknüpfung”. In this example we will see how to use untrained Bloom filters to match data.\n\nLoading the data\nFirst, we load our data into pandas.DataFrame objects. Here, the first records align, but the other two records should be swapped to have an aligned matching. We will use the toolkit to identify these matches.\n\nimport pandas as pd\n\ndf1 = pd.DataFrame(\n    {\n        \"first_name\": [\"Laura\", \"Kaspar\", \"Grete\"],\n        \"last_name\": [\"Daten\", \"Gorman\", \"Knopf\"],\n        \"gender\": [\"F\", \"M\", \"F\"],\n        \"date_of_birth\": [\"01/03/1977\", \"31/12/1975\", \"12/7/1981\"],\n        \"instrument\": [\"bass\", \"guitar\", \"drums\"],\n    }\n)\ndf2 = pd.DataFrame(\n    {\n        \"name\": [\"Laura Datten\", \"Greta Knopf\", \"Casper Goreman\"],\n        \"sex\": [\"female\", \"female\", \"male\"],\n        \"main_instrument\": [\"bass guitar\", \"percussion\", \"electric guitar\"],\n        \"birth_date\": [\"1977-03-23\", \"1981-07-12\", \"1975-12-31\"],\n    }\n)\n\n\n\n\n\n\n\nNote\n\n\n\nThese datasets don’t have the same column names or follow the same encodings, and there are several spelling mistakes in the names of the band members, as well as a typo in the dates.\nThankfully, the PPRL Toolkit is flexible enough to handle this!\n\n\n\n\nCreating and assigning a feature factory\nThe next step is to decide how to process each of the columns in our datasets.\nTo do this, we define a feature factory that maps column types to feature generation functions, and a column specification for each dataset mapping our columns to column types in the factory.\n\nfrom pprl.embedder import features\nfrom functools import partial\n\nfactory = dict(\n    name=features.gen_name_features,\n    sex=features.gen_sex_features,\n    misc=features.gen_misc_features,\n    dob=features.gen_dateofbirth_features,\n    instrument=partial(features.gen_misc_shingled_features, label=\"instrument\")\n)\nspec1 = dict(\n    first_name=\"name\",\n    last_name=\"name\",\n    gender=\"sex\",\n    instrument=\"instrument\",\n    date_of_birth=\"dob\",\n)\nspec2 = dict(name=\"name\", sex=\"sex\", main_instrument=\"instrument\", birth_date=\"dob\")\n\n\n\n\n\n\n\nTip\n\n\n\nThe feature generation functions, features.gen_XXX_features have sensible default parameters, but sometimes have to be passed in to the feature factory with different parameters, such as to set a feature label in the example above. There are two ways to achieve this. Either use functools.partial to set parameters (as above), or pass keyword arguments as a dictionary of dictionaries to the Embedder as ff_args.\n\n\n\n\nEmbedding the data\nWith our specifications sorted out, we can get to creating our Bloom filter embedding. Before doing so, we need to decide on two parameters: the size of the filter and the number of hashes. By default, these are 1024 and 2, respectively.\nOnce we’ve decided, we can create our Embedder instance and use it to embed our data with their column specifications.\n\nfrom pprl.embedder.embedder import Embedder\n\nembedder = Embedder(factory, bf_size=1024, num_hashes=2)\n\nedf1 = embedder.embed(df1, colspec=spec1, update_thresholds=True)\nedf2 = embedder.embed(df2, colspec=spec2, update_thresholds=True)\n\nIf we take a look at one of these embedded datasets, we can see that it has a whole bunch of new columns. There is a _features column for each of the original columns containing their pre-embedding string features, and there’s an all_features column that combines the features. Then there are three additional columns: bf_indices, bf_norms and thresholds.\n\nedf1.columns\n\nIndex(['first_name', 'last_name', 'gender', 'date_of_birth', 'instrument',\n       'first_name_features', 'last_name_features', 'gender_features',\n       'instrument_features', 'date_of_birth_features', 'all_features',\n       'bf_indices', 'bf_norms', 'thresholds'],\n      dtype='object')\n\n\nThe bf_indices column contains the Bloom filters, represented compactly as a list of non-zero indices for each record.\n\nprint(edf1.bf_indices[0])\n\n[2, 262, 903, 646, 9, 526, 654, 272, 15, 146, 17, 532, 531, 282, 667, 413, 670, 544, 288, 931, 292, 808, 937, 172, 942, 559, 816, 691, 820, 567, 56, 823, 440, 60, 61, 318, 319, 320, 444, 577, 836, 583, 332, 77, 590, 972, 465, 82, 211, 468, 84, 338, 851, 600, 593, 218, 861, 613, 871, 744, 238, 367, 881, 758, 890, 379, 1021, 763]\n\n\nThe bf_norms column contains the norm of each Bloom filter with respect to the Soft Cosine Measure (SCM) matrix. In this case since we are using an untrained model, the SCM matrix is an identity matrix, and the norm is just the Euclidean norm of the Bloom filter represented as a binary vector, which is equal to np.sqrt(len(bf_indices[i])) for record i. The norm is used to scale the similarity measures so that they take values between -1 and 1.\nThe thresholds column is calculated to provide, for each record, a threshold similarity score below which it will not be matched. It’s like a reserve price in an auction – it stops a record being matched to another record when the similarity isn’t high enough. This is an innovative feature of our method; other linkage methods typically only have one global threshold score for the entire dataset.\n\nprint(edf1.loc[:,[\"bf_norms\",\"thresholds\"]])\nprint(edf2.loc[:,[\"bf_norms\",\"thresholds\"]])\n\n   bf_norms  thresholds\n0  8.246211    0.114332\n1  9.055386    0.143159\n2  8.485281    0.143159\n    bf_norms  thresholds\n0   9.695360    0.294345\n1   9.380832    0.157014\n2  10.862781    0.294345\n\n\n\n\n\nThe processed features\nLet’s take a look at how the features are processed into small text strings (shingles) before being hashed into the Bloom filter. The first record in the first dataset is the same person as the first record in the second dataset, although the data is not identical, so we can compare the processed features for these records to see how pprl puts them into a format where they can be compared.\nFirst, we’ll look at date of birth:\n\nprint(edf1.date_of_birth_features[0])\nprint(edf2.birth_date_features[0])\n\n['day&lt;01&gt;', 'month&lt;03&gt;', 'year&lt;1977&gt;']\n['day&lt;23&gt;', 'month&lt;03&gt;', 'year&lt;1977&gt;']\n\n\nPython can parse the different formats easily. Although the dates are slightly different in the dataset, the year and month will still match, even though the day will not.\nThen we’ll look at name:\n\nprint(edf1.first_name_features[0] + edf1.last_name_features[0])\nprint(edf2.name_features[0])\n\n['_l', 'la', 'au', 'ur', 'ra', 'a_', '_la', 'lau', 'aur', 'ura', 'ra_', '_d', 'da', 'at', 'te', 'en', 'n_', '_da', 'dat', 'ate', 'ten', 'en_']\n['_l', 'la', 'au', 'ur', 'ra', 'a_', '_d', 'da', 'at', 'tt', 'te', 'en', 'n_', '_la', 'lau', 'aur', 'ura', 'ra_', '_da', 'dat', 'att', 'tte', 'ten', 'en_']\n\n\nThe two datasets store the names differently, but this doesn’t matter for the Bloom filter method because it treats each record like a bag of features. By default, the name processor produces 2-grams and 3-grams.\nThe sex processing function just converts different formats to lowercase and takes the first letter. This will often be enough:\n\nprint(edf1.gender_features[0])\nprint(edf2.sex_features[0])\n\n['sex&lt;f&gt;']\n['sex&lt;f&gt;']\n\n\nFinally, we’ll see how our instrument feature function (partial(features.gen_misc_shingled_features, label=\"instrument\")) processed the data:\n\nprint(edf1.instrument_features[0])\nprint(edf2.main_instrument_features[0])\n\n['instrument&lt;_b&gt;', 'instrument&lt;ba&gt;', 'instrument&lt;as&gt;', 'instrument&lt;ss&gt;', 'instrument&lt;s_&gt;', 'instrument&lt;_ba&gt;', 'instrument&lt;bas&gt;', 'instrument&lt;ass&gt;', 'instrument&lt;ss_&gt;']\n['instrument&lt;_b&gt;', 'instrument&lt;ba&gt;', 'instrument&lt;as&gt;', 'instrument&lt;ss&gt;', 'instrument&lt;s_&gt;', 'instrument&lt;_g&gt;', 'instrument&lt;gu&gt;', 'instrument&lt;ui&gt;', 'instrument&lt;it&gt;', 'instrument&lt;ta&gt;', 'instrument&lt;ar&gt;', 'instrument&lt;r_&gt;', 'instrument&lt;_ba&gt;', 'instrument&lt;bas&gt;', 'instrument&lt;ass&gt;', 'instrument&lt;ss_&gt;', 'instrument&lt;_gu&gt;', 'instrument&lt;gui&gt;', 'instrument&lt;uit&gt;', 'instrument&lt;ita&gt;', 'instrument&lt;tar&gt;', 'instrument&lt;ar_&gt;']\n\n\nSetting the label argument was important to ensure that the shingles match (and are hashed to the same slots) because the default behaviour of the function is to use the column name as a label: since the two columns have different names, the default wouldn’t have allowed the features to match to each other.\n\n\nPerforming the linkage\nWe can now perform the linkage by comparing these Bloom filter embeddings. We use the Soft Cosine Measure (which in this untrained model, is equivalent to a normal cosine similarity metric) to calculate record-wise similarity and an adapted Hungarian algorithm to match the records based on those similarities.\n\nsimilarities = embedder.compare(edf1, edf2)\nsimilarities\n\nSimilarityArray([[0.80050047, 0.10341754, 0.10047246],\n                 [0.34170424, 0.16480856, 0.63029481],\n                 [0.12155416, 0.54020787, 0.11933984]])\n\n\nThis SimilarityArray object is an augmented numpy.ndarray that can perform our matching. The matching itself can optionally be called with an absolute threshold score, but it doesn’t need one.\n\nmatching = similarities.match()\nmatching\n\n(array([0, 1, 2]), array([0, 2, 1]))\n\n\nSo, all three of the records in each dataset were matched correctly. Excellent!",
    "crumbs": [
      "About",
      "Docs",
      "Tutorials",
      "Exploring a simple linkage example"
    ]
  },
  {
    "objectID": "docs/tutorials/in-the-cloud.html",
    "href": "docs/tutorials/in-the-cloud.html",
    "title": "Working in the cloud",
    "section": "",
    "text": "This tutorial provides an overview of how to use the PPRL Toolkit on Google Cloud Platform (GCP). We go over how to assemble and assign roles in a linkage team, how to set up everybody’s projects, and end with executing the linkage itself.\nAbove is a diagram showing the PPRL cloud architecture. The cloud demo uses a Google Cloud Platform (GCP) Confidential Space compute instance, which is a virtual machine (VM) using AMD Secure Encrypted Virtualisation (AMD-SEV) technology to encrypt data in-memory. The Confidential Space VM can also provide cryptographically signed documents, called attestations, which the server can use to prove that it is running in a secure environment before gaining access to data.",
    "crumbs": [
      "About",
      "Docs",
      "Tutorials",
      "Working in the cloud"
    ]
  },
  {
    "objectID": "docs/tutorials/in-the-cloud.html#assembling-a-linkage-team",
    "href": "docs/tutorials/in-the-cloud.html#assembling-a-linkage-team",
    "title": "Working in the cloud",
    "section": "Assembling a linkage team",
    "text": "Assembling a linkage team\nThere are four roles to fill in any PPRL project: two data-owning parties, a workload author, and a workload operator. A workload is how we refer to the resources for the linkage operation itself (i.e. the containerised linkage code and the environment in which to run it.)\nThese roles need not be fulfilled by four separate people. It is perfectly possible to perform PPRL on your own, or perhaps you are working under a trust model that allows one of the data-owning parties to author the workload while the other is the operator.\n\n\n\n\n\n\nTip\n\n\n\nIn fact, the PPRL Toolkit is set up to allow any configuration of these roles among up to four people.\n\n\nIn any case, you must decide who will be doing what from the outset. Each role comes with different responsibilities, but all roles require a GCP account and access to the gcloud command-line tool. Additionally, everyone in the linkage project will need to install the PPRL Toolkit.\n\nData-owning party\nOften referred to as just a party, a data owner is responsible for the storage and preparation of some confidential data. During set-up, each party also sets up a storage bucket, a key management service, and a workload identity pool that allows the party to share permissions with the server during the linkage operation.\nThey create a Bloom filter embedding of their confidential data using an agreed configuration, and then upload that to GCP for processing. Once the workload operator is finished, the parties are able to retrieve their linkage results.\n\n\nWorkload author\nThe workload author is responsible for building a Docker image containing the cloud-based linkage code and uploading it to a GCP Artifact Registry. This image is the workload to be run by the operator.\n\n\nWorkload operator\nThe workload operator runs the linkage itself using some embedded data from the parties and an image from the author. They are responsible for setting up and running a Confidential Space in which to perform the linkage. This setting ensures that nobody ever has access to all the data at once, and that the data can only be accessed via the linkage code itself.",
    "crumbs": [
      "About",
      "Docs",
      "Tutorials",
      "Working in the cloud"
    ]
  },
  {
    "objectID": "docs/tutorials/in-the-cloud.html#creating-your-gcp-projects",
    "href": "docs/tutorials/in-the-cloud.html#creating-your-gcp-projects",
    "title": "Working in the cloud",
    "section": "Creating your GCP projects",
    "text": "Creating your GCP projects\nOnce you have decided who will be playing which role(s), you need to decide on a naming structure and make some GCP projects. You will need a project for each member of the linkage project - not one for each role. The names of these projects will be used throughout the cloud implementation, from configuration files to buckets. As such, they need to be descriptive and unique.\n\n\n\n\n\n\nWarning\n\n\n\nSince Google Cloud bucket names must be globally unique, we highly recommend using a hash in your project names to ensure that they are unique. This will ensure that bucket names are also globally unique.\nOur aim is to create a globally unique name (and thus ID) for each project.\n\n\nFor example, say a UK bank and a US bank are looking to link some data on international transactions to fit a machine learning model to predict fraud. Then they might use us-eaglebank and uk-royalbank as their party names, which are succinct and descriptive. However, they are generic and rule out future PPRL projects with the same names.\nAs a remedy, they could make a short hash of their project description to create an identifier:\n$ echo -n \"pprl us-eaglebank uk-royalbank fraud apr 2024\" | sha256sum | cut -c 1-7\n4fb6720\nSo, our project names would be: uk-royalbank-4fb6720, us-eaglebank-4fb6720. If they had a third-party linkage administrator (authoring and operating the workload), they would have a project called something like admin-4fb6720.",
    "crumbs": [
      "About",
      "Docs",
      "Tutorials",
      "Working in the cloud"
    ]
  },
  {
    "objectID": "docs/tutorials/in-the-cloud.html#setting-up-your-projects",
    "href": "docs/tutorials/in-the-cloud.html#setting-up-your-projects",
    "title": "Working in the cloud",
    "section": "Setting up your projects",
    "text": "Setting up your projects\nOnce you have decided on a naming structure, it is time to create the GCP projects. Each project will need specific Identity and Access Management (IAM) roles granted to them by the project owner’s GCP Administrator. Which IAM roles depends on the linkage role they are playing. If someone is fulfilling more than one role, they should follow all the relevant sections below.\n\n\n\n\n\n\nTip\n\n\n\nIf you have Administrator permissions for your GCP project, you can grant these roles using the gcloud command-line tool:\ngcloud projects add-iam-policy-binding &lt;project-name&gt; \\\n  --member=user:&lt;user-email&gt; \\\n  --role=&lt;role-code&gt;\n\n\n\nData-owning parties\nEach data-owning party requires the following IAM roles:\n\n\n\n\n\n\n\n\nTitle\nCode\nPurpose\n\n\n\n\nCloud KMS Admin\nroles/cloudkms.admin\nManaging encryption keys\n\n\nIAM Workload Identity Pool Admin\nroles/iam.workloadIdentityPoolAdmin\nManaging an impersonation service\n\n\nService Usage Admin\nroles/serviceusage.serviceUsageAdmin\nManaging access to other APIs\n\n\nService Account Admin\nroles/iam.serviceAccountAdmin\nManaging a service account\n\n\nStorage Admin\nroles/storage.admin\nManaging a bucket for their data\n\n\n\n\n\nWorkload author\nThe workload author only requires one IAM role:\n\n\n\n\n\n\n\n\nTitle\nCode\nPurpose\n\n\n\n\nArtifact Registry Administrator\nroles/artifactregistry.admin\nManaging the registry for the workload\n\n\n\n\n\nWorkload operator\nThe workload operator requires three IAM roles:\n\n\n\n\n\n\n\n\nTitle\nCode\nPurpose\n\n\n\n\nCompute Admin\nroles/compute.admin\nManaging the virtual machine\n\n\nSecurity Admin\nroles/iam.securityAdmin\nAbility to set and get IAM policies\n\n\nStorage Admin\nroles/storage.admin\nManaging a shared bucket",
    "crumbs": [
      "About",
      "Docs",
      "Tutorials",
      "Working in the cloud"
    ]
  },
  {
    "objectID": "docs/tutorials/in-the-cloud.html#configuring-the-pprl-toolkit",
    "href": "docs/tutorials/in-the-cloud.html#configuring-the-pprl-toolkit",
    "title": "Working in the cloud",
    "section": "Configuring the PPRL Toolkit",
    "text": "Configuring the PPRL Toolkit\nNow your linkage team has its projects made up, you need to configure the PPRL Toolkit. This configuration tells the package where to look and what to call things; we do this with a single environment file containing a short collection of key-value pairs.\nWe have provided an example environment file in .env.example. Copy or rename that file to .env in the root of the PPRL Toolkit installation. Then, fill in your project details as necessary.\nFor our example above, let’s say the ONS will be the workload author and the US Census Bureau will be the workload operator. The environment file would look something like this:\nPARTY_1_PROJECT=uk-royalbank-4fb6720\nPARTY_1_KEY_VERSION=1\n\nPARTY_2_PROJECT=us-eaglebank-4fb6720\nPARTY_2_KEY_VERSION=1\n\nWORKLOAD_AUTHOR_PROJECT=uk-royalbank-4fb6720\nWORKLOAD_AUTHOR_PROJECT_REGION=europe-west2\n\nWORKLOAD_OPERATOR_PROJECT=us-eaglebank-4fb6720\nWORKLOAD_OPERATOR_PROJECT_ZONE=us-east4-a\n\n\n\n\n\n\nImportant\n\n\n\nYour environment file should be identical among all the members of your linkage project.",
    "crumbs": [
      "About",
      "Docs",
      "Tutorials",
      "Working in the cloud"
    ]
  },
  {
    "objectID": "docs/tutorials/in-the-cloud.html#creating-the-other-resources",
    "href": "docs/tutorials/in-the-cloud.html#creating-the-other-resources",
    "title": "Working in the cloud",
    "section": "Creating the other resources",
    "text": "Creating the other resources\nThe last step in setting up your linkage project is to create and configure the other resources on GCP. To make things straightforward for users, we have packaged up the steps to do this into a number of bash scripts. These scripts are located in the scripts/ directory and are numbered. You and your team must execute them from the scripts/ directory in their named order according to which role(s) each member is fulfilling in the linkage project.\n\n\n\n\n\n\nTip\n\n\n\nMake sure you have set up gcloud on the command line. Once you’ve installed it, log in and set the application default:\ngcloud auth login\ngcloud auth application-default login\n\n\n\nThe data-owning parties set up: a key encryption key; a bucket in which to store their encrypted data, data encryption key and results; a service account for accessing said bucket and key; and a workload identity pool to allow impersonations under stringent conditions.\nsh ./01-setup-party-resources.sh &lt;name-of-party-project&gt;\nThe workload operator sets up a bucket for the parties to put their (non-sensitive) attestation credentials, and a service account for running the workload.\nsh ./02-setup-workload-operator.sh\nThe workload author sets up an Artifact Registry on GCP, creates a Docker image and uploads that image to their registry.\nsh ./03-setup-workload-author.sh\nThe data-owning parties authorise the workload operator’s service account to use the workload identity pool to impersonate their service account in a Confidential Space.\nsh ./04-authorise-workload.sh &lt;name-of-party-project&gt;",
    "crumbs": [
      "About",
      "Docs",
      "Tutorials",
      "Working in the cloud"
    ]
  },
  {
    "objectID": "docs/tutorials/in-the-cloud.html#processing-and-uploading-the-results",
    "href": "docs/tutorials/in-the-cloud.html#processing-and-uploading-the-results",
    "title": "Working in the cloud",
    "section": "Processing and uploading the results",
    "text": "Processing and uploading the results\n\n\n\n\n\n\nImportant\n\n\n\nThis section only applies to data-owning parties. The workload author is finished now, and the workload operator should wait for this section to be completed before moving on to the next section.\n\n\n\nNow that all the cloud infrastructure has been set up, we are ready to start the first step in doing the actual linkage. That is, to create a Bloom filter embedding of their data, encrypt it, and upload that to GCP.\nFor users who prefer a graphical user interface, we have included a Flask app to handle the processing and uploading of data behind the scenes. This app will also be used to download the results once the linkage has completed.\nTo launch the app, run the following in your terminal:\npython -m flask --app src/pprl/app run\nYou should now be able to find the app in your browser of choice at 127.0.0.1:5000. It should look something like this:\n\n\n\nA screenshot of the app\n\n\nFrom here, the process to upload your data is as follows:\n\nChoose which party you are uploading for. Click Submit.\nSelect Upload local file and click Choose file to open your file browser. Navigate to and select your dataset. Click Submit.\nAssign types to each column in your dataset. Enter the agreed salt.\nClick Upload file to GCP.\n\n\n\n\n\n\n\nNote\n\n\n\nIf you choose to use the Flask app to process your data, you will use a set of defaults for processing the confidential data before it gets embedded. If you want more control, then you’ll have to agree an embedding configuration with the other data-owning party and do the processing directly.\n\n\nOnce you have worked through the selection, processing, and GCP upload portions of the app, you will be at a holding page. This page can be updated by clicking the button, and when your results are ready you will be taken to another page where you can download them.",
    "crumbs": [
      "About",
      "Docs",
      "Tutorials",
      "Working in the cloud"
    ]
  },
  {
    "objectID": "docs/tutorials/in-the-cloud.html#running-the-linkage",
    "href": "docs/tutorials/in-the-cloud.html#running-the-linkage",
    "title": "Working in the cloud",
    "section": "Running the linkage",
    "text": "Running the linkage\n\n\n\n\n\n\nImportant\n\n\n\nThis section only applies to the workload operator.\n\n\nOnce the data-owning parties have uploaded their processed data, you are able to begin the linkage. To do so, run the 05-run-workload.sh bash script from scripts/:\ncd /path/to/pprl_toolkit/scripts\nsh ./05-run-workload.sh\nYou can follow the progress of the workload from the Logs Explorer on GCP. Once it is complete, the data-owning parties will be able to download their results.",
    "crumbs": [
      "About",
      "Docs",
      "Tutorials",
      "Working in the cloud"
    ]
  },
  {
    "objectID": "docs/tutorials/run-through.html",
    "href": "docs/tutorials/run-through.html",
    "title": "Embedder API run-through",
    "section": "",
    "text": "This article shows the main classes, methods and functionality of the Embedder API.\nFirst, we’ll import a few modules, including:\nimport os\nimport numpy as np\nimport pandas as pd\n\nfrom pprl import EmbeddedDataFrame, Embedder, config\nfrom pprl.embedder import features as feat",
    "crumbs": [
      "About",
      "Docs",
      "Tutorials",
      "Embedder API run-through"
    ]
  },
  {
    "objectID": "docs/tutorials/run-through.html#data-set-up",
    "href": "docs/tutorials/run-through.html#data-set-up",
    "title": "Embedder API run-through",
    "section": "Data set-up",
    "text": "Data set-up\nFor this demo we’ll create a really minimal pair of datasets. Notice that they don’t have to have the same structure or field names.\n\ndf1 = pd.DataFrame(\n    dict(\n        id=[1,2,3],\n        forename=[\"Henry\", \"Sally\", \"Ina\"],\n        surname = [\"Tull\", \"Brown\", \"Lawrey\"],\n        dob=[\"\", \"2/1/2001\", \"4/10/1995\"],\n        gender=[\"male\", \"Male\", \"Female\"],\n        county=[\"\", np.NaN, \"County Durham\"]\n    )\n)\n\ndf2 = pd.DataFrame(\n    dict(\n        personid=[4,5,6],\n        full_name=[\"Harry Tull\", \"Sali Brown\", \"Ina Laurie\"],\n        date_of_birth=[\"2/1/2001\", \"2/1/2001\", \"4/11/1995\"],\n        sex=[\"M\", \"M\", \"F\"],\n        county=[\"Rutland\", \"Powys\", \"Durham\"]\n    )\n)\n\nFeatures are extracted as different kinds of string objects from each field, ready to be hash embedded into the Bloom filters. We need to specify the feature extraction functions we’ll need.\nIn this case we’ll need one extractor for names, one for dates of birth, and one for sex/gender records. We create a dict with the functions we need. We create another dict to store any keyword arguments we want to pass in to each function (in this case we use all the default arguments so the keyword argument dictionaries are empty):\n\nfeature_factory = dict(\n    name=feat.gen_name_features,\n    dob=feat.gen_dateofbirth_features,\n    sex=feat.gen_sex_features,\n    misc=feat.gen_misc_features\n)\n\nff_args = dict(name={}, sex={}, dob={})",
    "crumbs": [
      "About",
      "Docs",
      "Tutorials",
      "Embedder API run-through"
    ]
  },
  {
    "objectID": "docs/tutorials/run-through.html#embedding",
    "href": "docs/tutorials/run-through.html#embedding",
    "title": "Embedder API run-through",
    "section": "Embedding",
    "text": "Embedding\nNow we can create an Embedder object. We want our Bloom filter vectors to have a length of 1024 elements, and we choose to hash each feature two times. These choices seem to work ok, but we haven’t explored them systematically.\n\nembedder = Embedder(feature_factory,\n                    ff_args,\n                    bf_size = 2**10,\n                    num_hashes=2,\n                    )\n\nNow we can hash embed the dataset into an EmbeddedDataFrame (EDF). For this we need to pass a column specification colspec that maps each column of the data into the feature_factory functions. Any columns not mapped will not contribute to the embedding.\n\nedf1 = embedder.embed(\n    df1, colspec=dict(forename=\"name\", surname=\"name\", dob=\"dob\", gender=\"sex\", county=\"misc\")\n)\nedf2 = embedder.embed(\n    df2, colspec=dict(full_name=\"name\", date_of_birth=\"dob\", sex=\"sex\", county=\"misc\")\n)\n\nprint(edf1)\nprint(edf2)\n\n   id forename surname        dob  gender         county  \\\n0   1    Henry    Tull               male                  \n1   2    Sally   Brown   2/1/2001    Male            NaN   \n2   3      Ina  Lawrey  4/10/1995  Female  County Durham   \n\n                                   forename_features  \\\n0  [_h, he, en, nr, ry, y_, _he, hen, enr, nry, ry_]   \n1  [_s, sa, al, ll, ly, y_, _sa, sal, all, lly, ly_]   \n2                    [_i, in, na, a_, _in, ina, na_]   \n\n                                    surname_features  \\\n0           [_t, tu, ul, ll, l_, _tu, tul, ull, ll_]   \n1  [_b, br, ro, ow, wn, n_, _br, bro, row, own, wn_]   \n2  [_l, la, aw, wr, re, ey, y_, _la, law, awr, wr...   \n\n                       dob_features gender_features          county_features  \\\n0                                []        [sex&lt;m&gt;]                            \n1  [day&lt;02&gt;, month&lt;01&gt;, year&lt;2001&gt;]        [sex&lt;m&gt;]                            \n2  [day&lt;04&gt;, month&lt;10&gt;, year&lt;1995&gt;]        [sex&lt;f&gt;]  [county&lt;county durham&gt;]   \n\n                                        all_features  \\\n0  [_he, he, _t, ll, tul, ry_, l_, tu, ll_, y_, e...   \n1  [_br, wn_, ro, ll, al, ly, row, _b, y_, _sa, o...   \n2  [sex&lt;f&gt;, county&lt;county durham&gt;, na_, re, y_, a...   \n\n                                          bf_indices  bf_norms  \n0  [644, 773, 135, 776, 265, 778, 271, 402, 404, ...  6.244998  \n1  [129, 258, 130, 776, 523, 525, 398, 271, 671, ...  7.141428  \n2  [647, 394, 269, 13, 15, 532, 667, 28, 413, 155...  7.000000  \n   personid   full_name date_of_birth sex   county  \\\n0         4  Harry Tull      2/1/2001   M  Rutland   \n1         5  Sali Brown      2/1/2001   M    Powys   \n2         6  Ina Laurie     4/11/1995   F   Durham   \n\n                                  full_name_features  \\\n0  [_h, ha, ar, rr, ry, y_, _t, tu, ul, ll, l_, _...   \n1  [_s, sa, al, li, i_, _b, br, ro, ow, wn, n_, _...   \n2  [_i, in, na, a_, _l, la, au, ur, ri, ie, e_, _...   \n\n             date_of_birth_features sex_features    county_features  \\\n0  [day&lt;02&gt;, month&lt;01&gt;, year&lt;2001&gt;]     [sex&lt;m&gt;]  [county&lt;rutland&gt;]   \n1  [day&lt;02&gt;, month&lt;01&gt;, year&lt;2001&gt;]     [sex&lt;m&gt;]    [county&lt;powys&gt;]   \n2  [day&lt;04&gt;, month&lt;11&gt;, year&lt;1995&gt;]     [sex&lt;f&gt;]   [county&lt;durham&gt;]   \n\n                                        all_features  \\\n0  [_t, ll, tul, ry_, l_, county&lt;rutland&gt;, ar, tu...   \n1  [_br, wn_, i_, ro, li_, al, ali, row, _b, wn, ...   \n2  [uri, sex&lt;f&gt;, month&lt;11&gt;, na_, ur, ie, a_, au, ...   \n\n                                          bf_indices  bf_norms  \n0  [640, 130, 644, 135, 776, 778, 10, 271, 402, 5...  6.855655  \n1  [130, 523, 525, 398, 271, 152, 671, 803, 806, ...  7.000000  \n2  [646, 647, 394, 269, 15, 272, 531, 532, 665, 6...  6.928203",
    "crumbs": [
      "About",
      "Docs",
      "Tutorials",
      "Embedder API run-through"
    ]
  },
  {
    "objectID": "docs/tutorials/run-through.html#training",
    "href": "docs/tutorials/run-through.html#training",
    "title": "Embedder API run-through",
    "section": "Training",
    "text": "Training\nDiscuss this at this stage",
    "crumbs": [
      "About",
      "Docs",
      "Tutorials",
      "Embedder API run-through"
    ]
  },
  {
    "objectID": "docs/tutorials/run-through.html#computing-the-similarity-scores-and-the-matching",
    "href": "docs/tutorials/run-through.html#computing-the-similarity-scores-and-the-matching",
    "title": "Embedder API run-through",
    "section": "Computing the similarity scores and the matching",
    "text": "Computing the similarity scores and the matching\nNow we have two embedded datasets, we can compare them and compute all the pairwise Cosine similarity scores.\nFirst, we have to compute the vector norms of each Bloom vector (for scaling the Cosine similarity) and the thresholds (thresholds are explained here [link]). Computing the thresholds can be time-consuming for a larger dataset, because it essentially computes all pairwise comparisons of the data to itself.\n\n\n\n\n\n\n\n\n\n\npersonid\nfull_name\ndate_of_birth\nsex\ncounty\nfull_name_features\ndate_of_birth_features\nsex_features\ncounty_features\nall_features\nbf_indices\nbf_norms\nthresholds\n\n\n\n\n0\n4\nHarry Tull\n2/1/2001\nM\nRutland\n[_h, ha, ar, rr, ry, y_, _t, tu, ul, ll, l_, _...\n[day&lt;02&gt;, month&lt;01&gt;, year&lt;2001&gt;]\n[sex&lt;m&gt;]\n[county&lt;rutland&gt;]\n[_t, ll, tul, ry_, l_, county&lt;rutland&gt;, ar, tu...\n[640, 130, 644, 135, 776, 778, 10, 271, 402, 5...\n6.855655\n0.187541\n\n\n1\n5\nSali Brown\n2/1/2001\nM\nPowys\n[_s, sa, al, li, i_, _b, br, ro, ow, wn, n_, _...\n[day&lt;02&gt;, month&lt;01&gt;, year&lt;2001&gt;]\n[sex&lt;m&gt;]\n[county&lt;powys&gt;]\n[_br, wn_, i_, ro, li_, al, ali, row, _b, wn, ...\n[130, 523, 525, 398, 271, 152, 671, 803, 806, ...\n7.000000\n0.187541\n\n\n2\n6\nIna Laurie\n4/11/1995\nF\nDurham\n[_i, in, na, a_, _l, la, au, ur, ri, ie, e_, _...\n[day&lt;04&gt;, month&lt;11&gt;, year&lt;1995&gt;]\n[sex&lt;f&gt;]\n[county&lt;durham&gt;]\n[uri, sex&lt;f&gt;, month&lt;11&gt;, na_, ur, ie, a_, au, ...\n[646, 647, 394, 269, 15, 272, 531, 532, 665, 6...\n6.928203\n0.082479\n\n\n\n\n\n\n\n\nNB: there’s also a flag to compute these at the same time as the embedding, but it doesn’t by default because, depending on the workflow, you may wish to compute the norms and thresholds at different times (e.g. on the server).\nNow you can compute the similarities:\n\nsimilarities = embedder.compare(edf1,edf2)\n\nprint(similarities)\n\n[[0.60728442 0.09150181 0.        ]\n [0.2859526  0.78015612 0.08084521]\n [0.08335143 0.10204083 0.57735028]]\n\n\nFinally, you can compute the matching:\n\nmatching = similarities.match(abs_cutoff=0.5)\n\nprint(matching)\n\n(array([0, 1, 2]), array([0, 1, 2]))",
    "crumbs": [
      "About",
      "Docs",
      "Tutorials",
      "Embedder API run-through"
    ]
  },
  {
    "objectID": "docs/tutorials/run-through.html#serialisation-and-file-io",
    "href": "docs/tutorials/run-through.html#serialisation-and-file-io",
    "title": "Embedder API run-through",
    "section": "Serialisation and file I/O",
    "text": "Serialisation and file I/O\nThat’s how to do the workflow in one session. However, this demo follows a multi-stage workflow, so we need to be able to pass objects around. There are a couple of methods that enable file I/O and serialisation.\nFirst, the Embedder object itself needs to be written to file and loaded. The idea is to train it, share it to the data owning parties, and also to the matching server. For this purpose, it’s possible to pickle the entire Embedder object.\n\nembedder.to_pickle(\"embedder.pkl\")\n\nembedder_copy = Embedder.from_pickle(\"embedder.pkl\")\n\nThe copy has the same functionality as the original:\n\nsimilarities = embedder_copy.compare(edf1,edf2)\n\nprint(similarities)\n\n[[0.60728442 0.09150181 0.        ]\n [0.2859526  0.78015612 0.08084521]\n [0.08335143 0.10204083 0.57735028]]\n\n\nNB: This won’t work if two datasets were embedded with different Embedder instances, even if they’re identical. The compare() method checks for the same embedder object memory reference so it won’t work if one was embedded with the original and the other with the copy. The way to fix this is to re-initialise the EmbeddedDataFrame with the new Embedder object.\n\nedf2_copy = EmbeddedDataFrame(edf2, embedder_copy)\n\nIn this case, be careful that the Embedder is compatible with the Bloom filter vectors in the EDF (i.e. uses the same parameters and feature factories), because while you can refresh the norms and thresholds, you can’t refresh the ‘bf_indices’ without reembedding the data frame.",
    "crumbs": [
      "About",
      "Docs",
      "Tutorials",
      "Embedder API run-through"
    ]
  },
  {
    "objectID": "docs/tutorials/run-through.html#serialising-the-data",
    "href": "docs/tutorials/run-through.html#serialising-the-data",
    "title": "Embedder API run-through",
    "section": "Serialising the data",
    "text": "Serialising the data\nThe EDF objects are just a thin wrapper around pandas.DataFrame instances, so you can serialise to JSON using the normal methods.\n\nedf1.to_json(\"edf1.json\")\n\nedf1_copy = pd.read_json(\"edf1.json\")\n\nprint(isinstance(edf1_copy,EmbeddedDataFrame))\nprint(isinstance(edf1_copy,pd.DataFrame))\n\nFalse\nTrue\n\n\nThe bf_indices, bf_norms and thresholds columns will be preserved. However, this demotes the data frames back to normal pandas.DataFrame instances and loses the link to an Embedder instance.\nTo fix this, just re-initialise them:\n\nedf1_copy = EmbeddedDataFrame(edf1_copy, embedder_copy)",
    "crumbs": [
      "About",
      "Docs",
      "Tutorials",
      "Embedder API run-through"
    ]
  },
  {
    "objectID": "docs/tutorials/example-febrl.html",
    "href": "docs/tutorials/example-febrl.html",
    "title": "Linking the FEBRL datasets",
    "section": "",
    "text": "This tutorial shows how the package can be used locally to match the FEBRL datasets, included as example datasets in the recordlinkage package.\nimport os\nimport time\nfrom functools import partial\n\nimport pandas as pd\nimport numpy as np\nfrom recordlinkage.datasets import load_febrl4\n\nfrom pprl import EmbeddedDataFrame, Embedder, config\nfrom pprl.embedder import features as feat",
    "crumbs": [
      "About",
      "Docs",
      "Tutorials",
      "Linking the FEBRL datasets"
    ]
  },
  {
    "objectID": "docs/tutorials/example-febrl.html#load-the-data",
    "href": "docs/tutorials/example-febrl.html#load-the-data",
    "title": "Linking the FEBRL datasets",
    "section": "Load the data",
    "text": "Load the data\nThe datasets we are using are 5000 records across two datasets with no duplicates, and each of the records has a valid match in the other dataset.\nAfter loading the data, we can parse the true matched ID number from the indices.\n\nfeb4a, feb4b = load_febrl4()\n\nfeb4a[\"true_id\"] = (\n    feb4a.index.str.extract(\"^rec-([0-9]*)-\")\n    .iloc[:, 0].astype(\"int\")\n    .to_list()\n)\nfeb4b[\"true_id\"] = (\n    feb4b.index.str.extract(\"^rec-([0-9]*)-\")\n    .iloc[:, 0].astype(\"int\")\n    .to_list()\n)",
    "crumbs": [
      "About",
      "Docs",
      "Tutorials",
      "Linking the FEBRL datasets"
    ]
  },
  {
    "objectID": "docs/tutorials/example-febrl.html#create-a-feature-factory",
    "href": "docs/tutorials/example-febrl.html#create-a-feature-factory",
    "title": "Linking the FEBRL datasets",
    "section": "Create a feature factory",
    "text": "Create a feature factory\nDefine the feature processing functions that convert each field into a string so that they can be hashed into the Bloom filter. The dictionary of functions is called feature_factory.\nIf we want to pass the feature factory functions into the embedder with non-default parameters, we have two options:\n\nPass a dictionary of dictionaries of keyword arguments as an optional ff_args parameter (e.g. ff_args = {\"dob\": {\"dayfirst\": False, \"yearfirst\": True}}))\nUse functools.partial(), as we have below.\n\n\nfeature_factory = dict(\n    name=feat.gen_name_features,\n    dob=partial(feat.gen_dateofbirth_features, dayfirst=False, yearfirst=True),\n    misc=feat.gen_misc_features,\n    address=partial(\n        feat.gen_misc_shingled_features, label=\"addr\", ngram_length=[4]\n    ),\n)",
    "crumbs": [
      "About",
      "Docs",
      "Tutorials",
      "Linking the FEBRL datasets"
    ]
  },
  {
    "objectID": "docs/tutorials/example-febrl.html#initialise-the-embedder-instance",
    "href": "docs/tutorials/example-febrl.html#initialise-the-embedder-instance",
    "title": "Linking the FEBRL datasets",
    "section": "Initialise the embedder instance",
    "text": "Initialise the embedder instance\nThis instance embeds each feature twice into a Bloom filter of length 1024.\n\nembedder = Embedder(feature_factory, bf_size=1024, num_hashes=2)",
    "crumbs": [
      "About",
      "Docs",
      "Tutorials",
      "Linking the FEBRL datasets"
    ]
  },
  {
    "objectID": "docs/tutorials/example-febrl.html#embed-the-datasets",
    "href": "docs/tutorials/example-febrl.html#embed-the-datasets",
    "title": "Linking the FEBRL datasets",
    "section": "Embed the datasets",
    "text": "Embed the datasets\nThe column specification colspec is a dictionary that tells the embedder how to map feature-processing functions to columns of the data. Notice that we can map more than one column to the same function. This means that we can easily handle cases where fields we want to compare all together span several columns.\nThis process makes our Bloom filter robust to inconsistencies where, for example, surname and given name may be swapped, or addresses may be coded inconsistently.\n\n\n\n\n\n\nNote\n\n\n\nTo ensure that different feature types hash into different buckets, the functions gen_misc_features() and gen_misc_shingled_features() will add a label to parsed string features, which will default to the data column name.\nFor example, to ensure suburb doesn’t collide with state (if they happened to be the same), gen_misc_features() would encode each of their tokens as suburb&lt;token&gt; and state&lt;token&gt;, respectively. If you want to map different columns into the same feature, such as address below, you can set the label explicitly when passing the function to the embedder.\n\n\n\ncolspec = dict(\n    given_name=\"name\",\n    surname=\"name\",\n    date_of_birth=\"dob\",\n    street_number=\"misc\",\n    state=\"misc\",\n    soc_sec_id=\"misc\",\n    postcode=\"misc\",\n    suburb=\"misc\",\n    address_1=\"address\",\n    address_2=\"address\",\n)\n\nedf1 = embedder.embed(feb4a, colspec=colspec)\nedf2 = embedder.embed(feb4b, colspec=colspec)\n\nStore the embedded datasets and their embedder to file.\n\nedf1.to_json(\"party1_data.json\")\nedf2.to_json(\"party2_data.json\")\nembedder.to_pickle(\"embedder.pkl\")",
    "crumbs": [
      "About",
      "Docs",
      "Tutorials",
      "Linking the FEBRL datasets"
    ]
  },
  {
    "objectID": "docs/tutorials/example-febrl.html#calculate-similarity",
    "href": "docs/tutorials/example-febrl.html#calculate-similarity",
    "title": "Linking the FEBRL datasets",
    "section": "Calculate similarity",
    "text": "Calculate similarity\nCompute the row thresholds to provide a lower bound on matching similarity scores for each row. This operation is the most computationally intensive part of the whole process.\n\nstart = time.time()\nedf1.update_thresholds()\nedf2.update_thresholds()\nend = time.time()\n\nprint(f\"Updating thresholds took {end - start:.2f} seconds\")\n\nUpdating thresholds took 8.35 seconds\n\n\nCompute the matrix of similarity scores.\n\nsimilarity_scores = embedder.compare(edf1,edf2)",
    "crumbs": [
      "About",
      "Docs",
      "Tutorials",
      "Linking the FEBRL datasets"
    ]
  },
  {
    "objectID": "docs/tutorials/example-febrl.html#compute-a-match",
    "href": "docs/tutorials/example-febrl.html#compute-a-match",
    "title": "Linking the FEBRL datasets",
    "section": "Compute a match",
    "text": "Compute a match\nUse the similarity scores to compute a match, using the Hungarian algorithm. First, we compute the match with the row thresholds.\n\nmatching = similarity_scores.match(require_thresholds=True)\n\nUsing the true IDs, evaluate the precision and recall of the match.\n\ndef get_results(edf1, edf2, matching):\n    \"\"\"Get the results for a given matching.\"\"\"\n\n    trueids_matched1 = edf1.iloc[matching[0], edf1.columns.get_loc(\"true_id\")]\n    trueids_matched2 = edf2.iloc[matching[1], edf2.columns.get_loc(\"true_id\")]\n\n    nmatches = len(matching[0])\n    truepos = sum(map(np.equal, trueids_matched1, trueids_matched2))\n    falsepos = nmatches - truepos\n\n    print(\n        f\"True pos: {truepos} | False pos: {falsepos} | \"\n        f\"Precision: {truepos / nmatches:.1%} | Recall: {truepos / 5000:.1%}\"\n    )\n\n    return nmatches, truepos, falsepos\n\n_ = get_results(edf1, edf2, matching)\n\nTrue pos: 4969 | False pos: 0 | Precision: 100.0% | Recall: 99.4%\n\n\nThen, we compute the match without using the row thresholds, calculating the same performance metrics:\n\nmatching = similarity_scores.match(require_thresholds=False)\n_ = get_results(edf1, edf2, matching)\n\nTrue pos: 5000 | False pos: 0 | Precision: 100.0% | Recall: 100.0%\n\n\nWithout using the row thresholds, the number of false positives is larger, but the recall is much better. For some uses this balance may be preferable.\nIn testing, the use of local row thresholds provides a better trade-off between precision and recall, compared to using a single absolute threshold. It has the additional advantage, in a privacy-preserving setting, of being automatic and not requiring clerical review to set the level.",
    "crumbs": [
      "About",
      "Docs",
      "Tutorials",
      "Linking the FEBRL datasets"
    ]
  },
  {
    "objectID": "docs/tutorials/index.html",
    "href": "docs/tutorials/index.html",
    "title": "Tutorials",
    "section": "",
    "text": "These tutorials walk you through some of the essential workflows for pprl. The purpose of these documents is for you to learn how to use the pprl package for your own linkage projects.\n\n\n\n\n\n\n\n\n\n\nTitle\n\n\nDescription\n\n\nReading Time\n\n\n\n\n\n\nEmbedder API run-through\n\n\n\n\n\n5 min\n\n\n\n\nExploring a simple linkage example\n\n\n\n\n\n6 min\n\n\n\n\nLinking the FEBRL datasets\n\n\nUsing PPRL locally to link two well-known datasets\n\n\n4 min\n\n\n\n\nWorking in the cloud\n\n\nGet you and your collaborators performing linkage in the cloud \n\n\n11 min\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "About",
      "Docs",
      "Tutorials"
    ]
  },
  {
    "objectID": "docs/reference/local.html",
    "href": "docs/reference/local.html",
    "title": "local",
    "section": "",
    "text": "matching.local\nFunctions for performing matching locally.\n\n\n\n\n\nName\nDescription\n\n\n\n\nbuild_local_file_paths\nConstruct the paths for the input and output datasets for a party.\n\n\nload_embedder\nLoad an embedder from a pickle in the local data directory.\n\n\n\n\n\nmatching.local.build_local_file_paths(party)\nConstruct the paths for the input and output datasets for a party.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nparty\nstr\nName of the party.\nrequired\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nstr\nLocation of the party data.\n\n\nstr\nLocation to put the party results.\n\n\n\n\n\n\n\nmatching.local.load_embedder()\nLoad an embedder from a pickle in the local data directory.\n\n\n\n\n\nType\nDescription\n\n\n\n\npprl.embedder.embedder.Embedder\nReformed embedder instance.",
    "crumbs": [
      "About",
      "Docs",
      "API reference",
      "local"
    ]
  },
  {
    "objectID": "docs/reference/local.html#functions",
    "href": "docs/reference/local.html#functions",
    "title": "local",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nbuild_local_file_paths\nConstruct the paths for the input and output datasets for a party.\n\n\nload_embedder\nLoad an embedder from a pickle in the local data directory.\n\n\n\n\n\nmatching.local.build_local_file_paths(party)\nConstruct the paths for the input and output datasets for a party.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nparty\nstr\nName of the party.\nrequired\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nstr\nLocation of the party data.\n\n\nstr\nLocation to put the party results.\n\n\n\n\n\n\n\nmatching.local.load_embedder()\nLoad an embedder from a pickle in the local data directory.\n\n\n\n\n\nType\nDescription\n\n\n\n\npprl.embedder.embedder.Embedder\nReformed embedder instance.",
    "crumbs": [
      "About",
      "Docs",
      "API reference",
      "local"
    ]
  },
  {
    "objectID": "docs/reference/bloom_filters.html",
    "href": "docs/reference/bloom_filters.html",
    "title": "bloom_filters",
    "section": "",
    "text": "embedder.bloom_filters\nModule for the Bloom filter encoder.\n\n\n\n\n\nName\nDescription\n\n\n\n\nBloomFilterEncoder\nEncoder of tokens and features via hashing and a Bloom filter.\n\n\n\n\n\nembedder.bloom_filters.BloomFilterEncoder(self, size=1024, num_hashes=2, offset=0, salt=None)\nEncoder of tokens and features via hashing and a Bloom filter.\nThe process for creating a cryptographically secure Bloom filter encoding of a set of tokens is as follows:\n\nCompute the hash digest for your tokens\nConvert the digest bytes into integers\nMap the integer to a bloom filter vector (modulo the length of the vector)\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsize\nint\nSize of the Bloom filter. Defaults to 1024\n1024\n\n\nnum_hashes\nint\nNumber of hashes to perform. Defaults to two.\n2\n\n\noffset\nint\nOffset for Bloom filter indices to allow for masking. Defaults to zero.\n0\n\n\nsalt\nstr | None\nCryptographic salt appended to tokens prior to hashing.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nhash_function\nfunc\nHashing function (hashlib.sha256).\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nbloom_filter_vector\nConvert a feature vector into indices for a Bloom vector.\n\n\nbloom_filter_vector_collision_fraction\nConvert a feature vector and return its collision fraction.\n\n\n\n\n\nembedder.bloom_filters.BloomFilterEncoder.bloom_filter_vector(feature)\nConvert a feature vector into indices for a Bloom vector.\nThe index vector uses an optional offset for masking.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfeature\nlist[str]\nList of features to be converted.\nrequired\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nlist\nIndex values used to create the Bloom filter vector.\n\n\n\n\n\n\n&gt;&gt;&gt; bfe = BloomFilterEncoder()\n&gt;&gt;&gt; bfe.bloom_filter_vector([\"a\",\"b\",\"c\"])\n[334, 1013, 192, 381, 18, 720]\n\n\n\n\nembedder.bloom_filters.BloomFilterEncoder.bloom_filter_vector_collision_fraction(feature)\nConvert a feature vector and return its collision fraction.\nThe index vector uses an optional offset for masking.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfeature\nlist[str]\nList of features to be processed.\nrequired\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nlist\nIndex values used to create the Bloom filter vector.\n\n\nfloat\nProportion of repeated indices.\n\n\n\n\n\n\n&gt;&gt;&gt; bfe = BloomFilterEncoder()\n&gt;&gt;&gt; bfe.bloom_filter_vector_collision_fraction([\"a\",\"b\",\"c\"])\n([334, 1013, 192, 381, 18, 720], 0.0)",
    "crumbs": [
      "About",
      "Docs",
      "API reference",
      "bloom_filters"
    ]
  },
  {
    "objectID": "docs/reference/bloom_filters.html#classes",
    "href": "docs/reference/bloom_filters.html#classes",
    "title": "bloom_filters",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nBloomFilterEncoder\nEncoder of tokens and features via hashing and a Bloom filter.\n\n\n\n\n\nembedder.bloom_filters.BloomFilterEncoder(self, size=1024, num_hashes=2, offset=0, salt=None)\nEncoder of tokens and features via hashing and a Bloom filter.\nThe process for creating a cryptographically secure Bloom filter encoding of a set of tokens is as follows:\n\nCompute the hash digest for your tokens\nConvert the digest bytes into integers\nMap the integer to a bloom filter vector (modulo the length of the vector)\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsize\nint\nSize of the Bloom filter. Defaults to 1024\n1024\n\n\nnum_hashes\nint\nNumber of hashes to perform. Defaults to two.\n2\n\n\noffset\nint\nOffset for Bloom filter indices to allow for masking. Defaults to zero.\n0\n\n\nsalt\nstr | None\nCryptographic salt appended to tokens prior to hashing.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nhash_function\nfunc\nHashing function (hashlib.sha256).\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nbloom_filter_vector\nConvert a feature vector into indices for a Bloom vector.\n\n\nbloom_filter_vector_collision_fraction\nConvert a feature vector and return its collision fraction.\n\n\n\n\n\nembedder.bloom_filters.BloomFilterEncoder.bloom_filter_vector(feature)\nConvert a feature vector into indices for a Bloom vector.\nThe index vector uses an optional offset for masking.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfeature\nlist[str]\nList of features to be converted.\nrequired\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nlist\nIndex values used to create the Bloom filter vector.\n\n\n\n\n\n\n&gt;&gt;&gt; bfe = BloomFilterEncoder()\n&gt;&gt;&gt; bfe.bloom_filter_vector([\"a\",\"b\",\"c\"])\n[334, 1013, 192, 381, 18, 720]\n\n\n\n\nembedder.bloom_filters.BloomFilterEncoder.bloom_filter_vector_collision_fraction(feature)\nConvert a feature vector and return its collision fraction.\nThe index vector uses an optional offset for masking.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfeature\nlist[str]\nList of features to be processed.\nrequired\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nlist\nIndex values used to create the Bloom filter vector.\n\n\nfloat\nProportion of repeated indices.\n\n\n\n\n\n\n&gt;&gt;&gt; bfe = BloomFilterEncoder()\n&gt;&gt;&gt; bfe.bloom_filter_vector_collision_fraction([\"a\",\"b\",\"c\"])\n([334, 1013, 192, 381, 18, 720], 0.0)",
    "crumbs": [
      "About",
      "Docs",
      "API reference",
      "bloom_filters"
    ]
  },
  {
    "objectID": "docs/reference/features.html",
    "href": "docs/reference/features.html",
    "title": "features",
    "section": "",
    "text": "embedder.features\nFeature generation functions for various column types.\n\n\n\n\n\nName\nDescription\n\n\n\n\ngen_dateofbirth_features\nGenerate labelled date features from a series of dates of birth.\n\n\ngen_double_metaphone\nGenerate the double methaphones of a string.\n\n\ngen_features\nGenerate string features of various types.\n\n\ngen_misc_features\nGenerate miscellaneous categorical features for a series.\n\n\ngen_misc_shingled_features\nGenerate shingled labelled features.\n\n\ngen_name_features\nGenerate a features series for a series of names.\n\n\ngen_ngram\nGenerate n-grams from a set of tokens.\n\n\ngen_sex_features\nGenerate labelled sex features from a series of sexes.\n\n\ngen_skip_grams\nGenerate skip 2-grams from a set of tokens.\n\n\nsplit_string_underscore\nSplit and underwrap a string at typical punctuation marks.\n\n\n\n\n\nembedder.features.gen_dateofbirth_features(dob, dayfirst=True, yearfirst=False, default=[])\nGenerate labelled date features from a series of dates of birth.\nFeatures take the form [\"day&lt;dd&gt;\", \"month&lt;mm&gt;\", \"year&lt;YYYY&gt;\"]. Note that this feature generator can be used for any sort of date data, not just dates of birth.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndob\npandas.pandas.Series\nSeries of dates of birth.\nrequired\n\n\ndayfirst\nbool\nWhether the day comes first in the DOBs. Passed to pd.to_datetime() and defaults to True.\nTrue\n\n\nyearfirst\nbool\nWhether the year comes first in the DOBs. Passed to pd.to_datetime() and defaults to False.\nFalse\n\n\ndefault\nlist[str]\nDefault date to fill in missing data in feature (list) form. Default is the feature form of 2050-01-01.\n[]\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\npandas.pandas.Series\nSeries containing lists of date features.\n\n\n\n\n\n\n\nembedder.features.gen_double_metaphone(string)\nGenerate the double methaphones of a string.\nThis function is a generator containing all the possible, non-empty double metaphones of a given string, separated by spaces. This function uses the metaphone.doublemetaphone() function under the hood, ignoring any empty strings. See their repository for details.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nstring\nstr\nString from which to derive double metaphones.\nrequired\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nstr\nThe next double metaphone in the sequence.\n\n\n\n\n\n\n\nembedder.features.gen_features(string, ngram_length=[2, 3], use_gen_ngram=True, use_gen_skip_grams=False, use_double_metaphone=False)\nGenerate string features of various types.\nThis function is a generator capable of producing n-grams, skip 2-grams, and double metaphones from a single string. These outputs are referred to as features.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nstring\nstr\nBase string from which to generate features.\nrequired\n\n\nngram_length\nlist\nLengths of n-grams to make. Ignored if use_gen_ngram=False.\n[2, 3]\n\n\nuse_gen_ngram\nbool\nWhether to create n-grams. Default is True.\nTrue\n\n\nuse_gen_skip_grams\nbool\nWhether to create skip 2-grams. Default is False.\nFalse\n\n\nuse_double_metaphone\nbool\nWhether to create double metaphones. Default is False.\nFalse\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nstr\nThe next feature in the sequence.\n\n\n\n\n\n\n\nembedder.features.gen_misc_features(field, label=None)\nGenerate miscellaneous categorical features for a series.\nUseful for keeping raw columns in the linkage data. All features use a label and take the form [\"label&lt;option&gt;\"] except for missing data, which are coded as \"\".\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfield\npandas.pandas.Series\nSeries from which to generate our features.\nrequired\n\n\nlabel\nNone | str | typing.Hashable\nLabel for the series. By default, the name of the series is used if available. Otherwise, if not specified, misc is used.\nNone\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\npandas.pandas.Series\nSeries containing lists of miscellaneous features.\n\n\n\n\n\n\n\nembedder.features.gen_misc_shingled_features(field, ngram_length=[2, 3], use_gen_skip_grams=False, label=None)\nGenerate shingled labelled features.\nGenerate n-grams, with a label to distinguish them from (and ensure they’re hashed separately from) names. Like gen_name_features(), this function makes a call to gen_features() via pd.Series.apply().\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfield\npandas.pandas.Series\nSeries of string data.\nrequired\n\n\nngram_length\nlist\nShingle sizes to generate. By default [2, 3].\n[2, 3]\n\n\nuse_gen_skip_grams\nbool\nWhether to generate skip 2-grams. False by default.\nFalse\n\n\nlabel\nstr\nA label to differentiate from other shingled features. If field has no name, this defaults to zz.\nNone\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\npandas.pandas.Series\nSeries containing lists of shingled string features.\n\n\n\n\n\n\n\nembedder.features.gen_name_features(names, ngram_length=[2, 3], use_gen_ngram=True, use_gen_skip_grams=False, use_double_metaphone=False)\nGenerate a features series for a series of names.\nEffectively, this function is a call to pd.Series.apply() using our gen_features() string feature generator function.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nnames\npandas.pandas.Series\nSeries of names.\nrequired\n\n\nngram_length\nlist[int]\nLengths of n-grams to make. Ignored if use_gen_ngram=False.\n[2, 3]\n\n\nuse_gen_ngram\nbool\nWhether to create n-grams. Default is True.\nTrue\n\n\nuse_gen_skip_grams\nbool\nWhether to create skip 2-grams. Default is False.\nFalse\n\n\nuse_double_metaphone\nbool\nWhether to create double metaphones. Default is False.\nFalse\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\npandas.pandas.Series\nSeries containing lists of features.\n\n\n\n\n\n\n\nembedder.features.gen_ngram(split_tokens, ngram_length)\nGenerate n-grams from a set of tokens.\nThis is a generator function that contains a series of n-grams the size of the sliding window.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsplit_tokens\nlist\nAll the split-up tokens from which to form n-grams.\nrequired\n\n\nngram_length\nlist\nDesired lengths of n-grams. For examples, ngram_length=[2, 3] would generate all 2-grams and 3-grams.\nrequired\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nstr\nThe next n-gram in the sequence.\n\n\n\n\n\n\n\nembedder.features.gen_sex_features(sexes)\nGenerate labelled sex features from a series of sexes.\nFeatures take the form [\"sex&lt;option&gt;\"] or [\"\"] for missing data.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsexes\npandas.pandas.Series\nSeries of sex data.\nrequired\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\npandas.pandas.Series\nSeries containing lists of sex features.\n\n\n\n\n\n\n\nembedder.features.gen_skip_grams(split_tokens)\nGenerate skip 2-grams from a set of tokens.\nThis function is a generator that contains a series of skip 2-grams.\n\n\n&gt;&gt;&gt; string = \"dave james\"\n&gt;&gt;&gt; tokens = split_string_underscore(string)\n&gt;&gt;&gt; skips = list(gen_skip_grams(tokens))\n&gt;&gt;&gt; print(skips)\n[\"_a\", \"dv\", \"ae\", \"v_\", \"_a\", \"jm\", \"ae\", \"ms\", \"e_\"]\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsplit_tokens\nlist\nAll the split-up tokens from which to form skip 2-grams.\nrequired\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nstr\nThe next skip 2-gram in the sequence.\n\n\n\n\n\n\n\nembedder.features.split_string_underscore(string)\nSplit and underwrap a string at typical punctuation marks.\nCurrently, we split at any combination of spaces, dashes, dots, commas, or underscores.\n\n\n&gt;&gt;&gt; strings = (\"dave  william johnson\", \"Francesca__Hogan-O'Malley\")\n&gt;&gt;&gt; for string in strings:\n...     print(split_string_underscore(string))\n[\"_dave_\", \"_william_\", \"_johnson_\"]\n[\"_Francesca_\", \"_Hogan_\", \"_O'Malley_\"]\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nstring\nstr\nString to split.\nrequired\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nlist[str]\nList of the split and wrapped tokens.",
    "crumbs": [
      "About",
      "Docs",
      "API reference",
      "features"
    ]
  },
  {
    "objectID": "docs/reference/features.html#functions",
    "href": "docs/reference/features.html#functions",
    "title": "features",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\ngen_dateofbirth_features\nGenerate labelled date features from a series of dates of birth.\n\n\ngen_double_metaphone\nGenerate the double methaphones of a string.\n\n\ngen_features\nGenerate string features of various types.\n\n\ngen_misc_features\nGenerate miscellaneous categorical features for a series.\n\n\ngen_misc_shingled_features\nGenerate shingled labelled features.\n\n\ngen_name_features\nGenerate a features series for a series of names.\n\n\ngen_ngram\nGenerate n-grams from a set of tokens.\n\n\ngen_sex_features\nGenerate labelled sex features from a series of sexes.\n\n\ngen_skip_grams\nGenerate skip 2-grams from a set of tokens.\n\n\nsplit_string_underscore\nSplit and underwrap a string at typical punctuation marks.\n\n\n\n\n\nembedder.features.gen_dateofbirth_features(dob, dayfirst=True, yearfirst=False, default=[])\nGenerate labelled date features from a series of dates of birth.\nFeatures take the form [\"day&lt;dd&gt;\", \"month&lt;mm&gt;\", \"year&lt;YYYY&gt;\"]. Note that this feature generator can be used for any sort of date data, not just dates of birth.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndob\npandas.pandas.Series\nSeries of dates of birth.\nrequired\n\n\ndayfirst\nbool\nWhether the day comes first in the DOBs. Passed to pd.to_datetime() and defaults to True.\nTrue\n\n\nyearfirst\nbool\nWhether the year comes first in the DOBs. Passed to pd.to_datetime() and defaults to False.\nFalse\n\n\ndefault\nlist[str]\nDefault date to fill in missing data in feature (list) form. Default is the feature form of 2050-01-01.\n[]\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\npandas.pandas.Series\nSeries containing lists of date features.\n\n\n\n\n\n\n\nembedder.features.gen_double_metaphone(string)\nGenerate the double methaphones of a string.\nThis function is a generator containing all the possible, non-empty double metaphones of a given string, separated by spaces. This function uses the metaphone.doublemetaphone() function under the hood, ignoring any empty strings. See their repository for details.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nstring\nstr\nString from which to derive double metaphones.\nrequired\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nstr\nThe next double metaphone in the sequence.\n\n\n\n\n\n\n\nembedder.features.gen_features(string, ngram_length=[2, 3], use_gen_ngram=True, use_gen_skip_grams=False, use_double_metaphone=False)\nGenerate string features of various types.\nThis function is a generator capable of producing n-grams, skip 2-grams, and double metaphones from a single string. These outputs are referred to as features.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nstring\nstr\nBase string from which to generate features.\nrequired\n\n\nngram_length\nlist\nLengths of n-grams to make. Ignored if use_gen_ngram=False.\n[2, 3]\n\n\nuse_gen_ngram\nbool\nWhether to create n-grams. Default is True.\nTrue\n\n\nuse_gen_skip_grams\nbool\nWhether to create skip 2-grams. Default is False.\nFalse\n\n\nuse_double_metaphone\nbool\nWhether to create double metaphones. Default is False.\nFalse\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nstr\nThe next feature in the sequence.\n\n\n\n\n\n\n\nembedder.features.gen_misc_features(field, label=None)\nGenerate miscellaneous categorical features for a series.\nUseful for keeping raw columns in the linkage data. All features use a label and take the form [\"label&lt;option&gt;\"] except for missing data, which are coded as \"\".\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfield\npandas.pandas.Series\nSeries from which to generate our features.\nrequired\n\n\nlabel\nNone | str | typing.Hashable\nLabel for the series. By default, the name of the series is used if available. Otherwise, if not specified, misc is used.\nNone\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\npandas.pandas.Series\nSeries containing lists of miscellaneous features.\n\n\n\n\n\n\n\nembedder.features.gen_misc_shingled_features(field, ngram_length=[2, 3], use_gen_skip_grams=False, label=None)\nGenerate shingled labelled features.\nGenerate n-grams, with a label to distinguish them from (and ensure they’re hashed separately from) names. Like gen_name_features(), this function makes a call to gen_features() via pd.Series.apply().\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfield\npandas.pandas.Series\nSeries of string data.\nrequired\n\n\nngram_length\nlist\nShingle sizes to generate. By default [2, 3].\n[2, 3]\n\n\nuse_gen_skip_grams\nbool\nWhether to generate skip 2-grams. False by default.\nFalse\n\n\nlabel\nstr\nA label to differentiate from other shingled features. If field has no name, this defaults to zz.\nNone\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\npandas.pandas.Series\nSeries containing lists of shingled string features.\n\n\n\n\n\n\n\nembedder.features.gen_name_features(names, ngram_length=[2, 3], use_gen_ngram=True, use_gen_skip_grams=False, use_double_metaphone=False)\nGenerate a features series for a series of names.\nEffectively, this function is a call to pd.Series.apply() using our gen_features() string feature generator function.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nnames\npandas.pandas.Series\nSeries of names.\nrequired\n\n\nngram_length\nlist[int]\nLengths of n-grams to make. Ignored if use_gen_ngram=False.\n[2, 3]\n\n\nuse_gen_ngram\nbool\nWhether to create n-grams. Default is True.\nTrue\n\n\nuse_gen_skip_grams\nbool\nWhether to create skip 2-grams. Default is False.\nFalse\n\n\nuse_double_metaphone\nbool\nWhether to create double metaphones. Default is False.\nFalse\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\npandas.pandas.Series\nSeries containing lists of features.\n\n\n\n\n\n\n\nembedder.features.gen_ngram(split_tokens, ngram_length)\nGenerate n-grams from a set of tokens.\nThis is a generator function that contains a series of n-grams the size of the sliding window.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsplit_tokens\nlist\nAll the split-up tokens from which to form n-grams.\nrequired\n\n\nngram_length\nlist\nDesired lengths of n-grams. For examples, ngram_length=[2, 3] would generate all 2-grams and 3-grams.\nrequired\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nstr\nThe next n-gram in the sequence.\n\n\n\n\n\n\n\nembedder.features.gen_sex_features(sexes)\nGenerate labelled sex features from a series of sexes.\nFeatures take the form [\"sex&lt;option&gt;\"] or [\"\"] for missing data.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsexes\npandas.pandas.Series\nSeries of sex data.\nrequired\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\npandas.pandas.Series\nSeries containing lists of sex features.\n\n\n\n\n\n\n\nembedder.features.gen_skip_grams(split_tokens)\nGenerate skip 2-grams from a set of tokens.\nThis function is a generator that contains a series of skip 2-grams.\n\n\n&gt;&gt;&gt; string = \"dave james\"\n&gt;&gt;&gt; tokens = split_string_underscore(string)\n&gt;&gt;&gt; skips = list(gen_skip_grams(tokens))\n&gt;&gt;&gt; print(skips)\n[\"_a\", \"dv\", \"ae\", \"v_\", \"_a\", \"jm\", \"ae\", \"ms\", \"e_\"]\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsplit_tokens\nlist\nAll the split-up tokens from which to form skip 2-grams.\nrequired\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nstr\nThe next skip 2-gram in the sequence.\n\n\n\n\n\n\n\nembedder.features.split_string_underscore(string)\nSplit and underwrap a string at typical punctuation marks.\nCurrently, we split at any combination of spaces, dashes, dots, commas, or underscores.\n\n\n&gt;&gt;&gt; strings = (\"dave  william johnson\", \"Francesca__Hogan-O'Malley\")\n&gt;&gt;&gt; for string in strings:\n...     print(split_string_underscore(string))\n[\"_dave_\", \"_william_\", \"_johnson_\"]\n[\"_Francesca_\", \"_Hogan_\", \"_O'Malley_\"]\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nstring\nstr\nString to split.\nrequired\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nlist[str]\nList of the split and wrapped tokens.",
    "crumbs": [
      "About",
      "Docs",
      "API reference",
      "features"
    ]
  },
  {
    "objectID": "docs/reference/perform.html",
    "href": "docs/reference/perform.html",
    "title": "perform",
    "section": "",
    "text": "matching.perform\nFunctions for performing the matching itself.\n\n\n\n\n\nName\nDescription\n\n\n\n\nadd_private_index\nAdd anonymous match index to input datasets.\n\n\ncalculate_performance\nCalculate the performance of the match by counting the positives.\n\n\nperform_matching\nInitiate the data, get similarities, and match the rows.\n\n\n\n\n\nmatching.perform.add_private_index(df1, df2, match, size_assumed=10000, colname='private_index')\nAdd anonymous match index to input datasets.\nThe match index assigns indices to both matched and unmatched records, so that they are indistinguishable. It doesn’t leak any info about the other dataset.\nadd_private_index only works with unique one-to-one matches. This is because there is no way to match many-to-one without leaking information about the successful matches.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndf1\npandas.pandas.DataFrame\nA dataset.\nrequired\n\n\ndf2\npandas.pandas.DataFrame\nAnother dataset.\nrequired\n\n\nmatch\ntuple[numpy.numpy.ndarray, numpy.numpy.ndarray]\nA pair of matched indices, with no repeated indices.\nrequired\n\n\nsize_assumed\nint\nThe assumed maximum size of each dataset. Default is 10,000.\n10000\n\n\ncolname\nstr\nA column name for the new index. By default \"private_index\".\n'private_index'\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\ndf1, df2: pd.DataFrame\nThe same as input data, with private matching index added.\n\n\n\n\n\n\n\nmatching.perform.calculate_performance(data_1, data_2, match)\nCalculate the performance of the match by counting the positives.\nPerformance metrics are sent to the logger.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndata_1\npandas.pandas.DataFrame\nData frame for PARTY1.\nrequired\n\n\ndata_2\npandas.pandas.DataFrame\nData frame for PARTY2.\nrequired\n\n\nmatch\ntuple\nTuple of indices of matched pairs between the data frames.\nrequired\n\n\n\n\n\n\n\nmatching.perform.perform_matching(data_1, data_2, embedder)\nInitiate the data, get similarities, and match the rows.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndata_1\npandas.pandas.DataFrame\nData frame for PARTY1.\nrequired\n\n\ndata_2\npandas.pandas.DataFrame\nData frame for PARTY2.\nrequired\n\n\nembedder\npprl.embedder.embedder.Embedder\nInstance used to embed both data frames.\nrequired\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\npandas.pandas.DataFrame\nOutput for PARTY1.\n\n\npandas.pandas.DataFrame\nOutput for PARTY2.",
    "crumbs": [
      "About",
      "Docs",
      "API reference",
      "perform"
    ]
  },
  {
    "objectID": "docs/reference/perform.html#functions",
    "href": "docs/reference/perform.html#functions",
    "title": "perform",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nadd_private_index\nAdd anonymous match index to input datasets.\n\n\ncalculate_performance\nCalculate the performance of the match by counting the positives.\n\n\nperform_matching\nInitiate the data, get similarities, and match the rows.\n\n\n\n\n\nmatching.perform.add_private_index(df1, df2, match, size_assumed=10000, colname='private_index')\nAdd anonymous match index to input datasets.\nThe match index assigns indices to both matched and unmatched records, so that they are indistinguishable. It doesn’t leak any info about the other dataset.\nadd_private_index only works with unique one-to-one matches. This is because there is no way to match many-to-one without leaking information about the successful matches.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndf1\npandas.pandas.DataFrame\nA dataset.\nrequired\n\n\ndf2\npandas.pandas.DataFrame\nAnother dataset.\nrequired\n\n\nmatch\ntuple[numpy.numpy.ndarray, numpy.numpy.ndarray]\nA pair of matched indices, with no repeated indices.\nrequired\n\n\nsize_assumed\nint\nThe assumed maximum size of each dataset. Default is 10,000.\n10000\n\n\ncolname\nstr\nA column name for the new index. By default \"private_index\".\n'private_index'\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\ndf1, df2: pd.DataFrame\nThe same as input data, with private matching index added.\n\n\n\n\n\n\n\nmatching.perform.calculate_performance(data_1, data_2, match)\nCalculate the performance of the match by counting the positives.\nPerformance metrics are sent to the logger.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndata_1\npandas.pandas.DataFrame\nData frame for PARTY1.\nrequired\n\n\ndata_2\npandas.pandas.DataFrame\nData frame for PARTY2.\nrequired\n\n\nmatch\ntuple\nTuple of indices of matched pairs between the data frames.\nrequired\n\n\n\n\n\n\n\nmatching.perform.perform_matching(data_1, data_2, embedder)\nInitiate the data, get similarities, and match the rows.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndata_1\npandas.pandas.DataFrame\nData frame for PARTY1.\nrequired\n\n\ndata_2\npandas.pandas.DataFrame\nData frame for PARTY2.\nrequired\n\n\nembedder\npprl.embedder.embedder.Embedder\nInstance used to embed both data frames.\nrequired\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\npandas.pandas.DataFrame\nOutput for PARTY1.\n\n\npandas.pandas.DataFrame\nOutput for PARTY2.",
    "crumbs": [
      "About",
      "Docs",
      "API reference",
      "perform"
    ]
  },
  {
    "objectID": "docs/reference/utils.html",
    "href": "docs/reference/utils.html",
    "title": "utils",
    "section": "",
    "text": "app.utils\nUtility functions for the party-side app.\n\n\n\n\n\nName\nDescription\n\n\n\n\nassign_columns\nAssign columns from a form to collections.\n\n\ncheck_is_csv\nDetermine whether a file has the csv extension.\n\n\nconvert_dataframe_to_bf\nConvert a dataframe of features to a bloom filter.\n\n\ndownload_files\nSerialize, compress, and send a data frame with its embedder.\n\n\n\n\n\napp.utils.assign_columns(form, feature_funcs)\nAssign columns from a form to collections.\nAll columns belong to one of three collections: columns to drop, raw columns to keep, or a column feature factory specification.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nform\ndict\nForm from our column chooser page.\nrequired\n\n\nfeature_funcs\ndict\nMapping between column types and feature functions.\nrequired\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nlist[str]\nList of columns to drop.\n\n\nlist[str]\nList of columns to keep in their raw format.\n\n\ndict[str, func]\nMapping between column names and feature functions.\n\n\n\n\n\n\n\napp.utils.check_is_csv(path)\nDetermine whether a file has the csv extension.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npath\nstr\nPath to the file.\nrequired\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nbool\nWhether the file name follows the pattern {name}.csv or not.\n\n\n\n\n\n\n\napp.utils.convert_dataframe_to_bf(df, colspec, other_columns=None, salt='')\nConvert a dataframe of features to a bloom filter.\nConvert the columns to features based on the colspec. The features are then combined and converted to Bloom filter indices with the Bloom filter norm also calculated.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndf\npandas.pandas.DataFrame\nData frame of features.\nrequired\n\n\ncolspec\ndict\nDictionary designating columns in the data frame as particular feature types to be processed as appropriate.\nrequired\n\n\nother_columns\nNone | list\nColumns to be returned as they appear in the data in addition to bf_indices, bf_norms and thresholds.\nNone\n\n\nsalt\nstr\nCryptographic salt to add to tokens before hashing.\n''\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\npandas.pandas.DataFrame\nData frame of bloom-filtered data.\n\n\n\n\n\n\n\napp.utils.download_files(dataframe, embedder, party, archive='archive')\nSerialize, compress, and send a data frame with its embedder.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndataframe\npprl.embedder.embedder.EmbeddedDataFrame\nData frame to be downloaded.\nrequired\n\n\nembedder\npprl.embedder.embedder.Embedder\nEmbedder used to embed dataframe.\nrequired\n\n\nparty\nstr\nName of the party.\nrequired\n\n\narchive\nstr\nName of the archive. Default is \"archive\".\n'archive'\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nflask.flask.Response\nResponse containing a ZIP archive with the data frame and its embedder.",
    "crumbs": [
      "About",
      "Docs",
      "API reference",
      "utils"
    ]
  },
  {
    "objectID": "docs/reference/utils.html#functions",
    "href": "docs/reference/utils.html#functions",
    "title": "utils",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nassign_columns\nAssign columns from a form to collections.\n\n\ncheck_is_csv\nDetermine whether a file has the csv extension.\n\n\nconvert_dataframe_to_bf\nConvert a dataframe of features to a bloom filter.\n\n\ndownload_files\nSerialize, compress, and send a data frame with its embedder.\n\n\n\n\n\napp.utils.assign_columns(form, feature_funcs)\nAssign columns from a form to collections.\nAll columns belong to one of three collections: columns to drop, raw columns to keep, or a column feature factory specification.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nform\ndict\nForm from our column chooser page.\nrequired\n\n\nfeature_funcs\ndict\nMapping between column types and feature functions.\nrequired\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nlist[str]\nList of columns to drop.\n\n\nlist[str]\nList of columns to keep in their raw format.\n\n\ndict[str, func]\nMapping between column names and feature functions.\n\n\n\n\n\n\n\napp.utils.check_is_csv(path)\nDetermine whether a file has the csv extension.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npath\nstr\nPath to the file.\nrequired\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nbool\nWhether the file name follows the pattern {name}.csv or not.\n\n\n\n\n\n\n\napp.utils.convert_dataframe_to_bf(df, colspec, other_columns=None, salt='')\nConvert a dataframe of features to a bloom filter.\nConvert the columns to features based on the colspec. The features are then combined and converted to Bloom filter indices with the Bloom filter norm also calculated.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndf\npandas.pandas.DataFrame\nData frame of features.\nrequired\n\n\ncolspec\ndict\nDictionary designating columns in the data frame as particular feature types to be processed as appropriate.\nrequired\n\n\nother_columns\nNone | list\nColumns to be returned as they appear in the data in addition to bf_indices, bf_norms and thresholds.\nNone\n\n\nsalt\nstr\nCryptographic salt to add to tokens before hashing.\n''\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\npandas.pandas.DataFrame\nData frame of bloom-filtered data.\n\n\n\n\n\n\n\napp.utils.download_files(dataframe, embedder, party, archive='archive')\nSerialize, compress, and send a data frame with its embedder.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndataframe\npprl.embedder.embedder.EmbeddedDataFrame\nData frame to be downloaded.\nrequired\n\n\nembedder\npprl.embedder.embedder.Embedder\nEmbedder used to embed dataframe.\nrequired\n\n\nparty\nstr\nName of the party.\nrequired\n\n\narchive\nstr\nName of the archive. Default is \"archive\".\n'archive'\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nflask.flask.Response\nResponse containing a ZIP archive with the data frame and its embedder.",
    "crumbs": [
      "About",
      "Docs",
      "API reference",
      "utils"
    ]
  }
]